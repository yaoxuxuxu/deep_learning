{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a296724e",
   "metadata": {},
   "source": [
    "# 实验 Cross Entropy Loss的向量化实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ad1f84",
   "metadata": {},
   "source": [
    "## 读取MNIST数据集，并将其划分为train/val/test数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a4f88a",
   "metadata": {},
   "source": [
    "### MNIST数据集读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f545d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"加载MNIST数据集\"\"\"\n",
    "    labels_path = os.path.join(path, f'{kind}-labels-idx1-ubyte.gz')\n",
    "    images_path = os.path.join(path, f'{kind}-images-idx3-ubyte.gz')\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        struct.unpack('>II', lbpath.read(8))\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        struct.unpack('>IIII', imgpath.read(16))\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "\n",
    "# 数据集划分\n",
    "def data_split(images, labels, ratio):\n",
    "    \n",
    "    total_len = images.shape[0]\n",
    "    offset = int(total_len * ratio)\n",
    "    \n",
    "    val_img = images[:offset][:]\n",
    "    val_lb = labels[:offset]\n",
    "    \n",
    "    train_img = images[offset:][:]\n",
    "    train_lb = labels[offset:]\n",
    "    \n",
    "    return train_img, train_lb, val_img, val_lb    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27731d8f",
   "metadata": {},
   "source": [
    "### 按照 5:1:1 划分为训练集，验证集，测试集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "56e3880e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集图像格式为: (50000, 784) 训练集标签格式为: (50000,)\n",
      "验证集图像格式为: (10000, 784) 验证集标签格式为: (10000,)\n",
      "测试集图像格式为: (10000, 784) 测试集标签格式为: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# 读取训练集和测试集数据\n",
    "[images, labels] = load_mnist('./MNIST', kind='train')\n",
    "[test_img, test_lb] = load_mnist('./MNIST',kind='test')\n",
    "train_img, train_lb, val_img, val_lb = data_split(images, labels, 1/6)\n",
    "\n",
    "\n",
    "# 打印查看数据集格式\n",
    "print('训练集图像格式为:', train_img.shape, '训练集标签格式为:', train_lb.shape)\n",
    "print('验证集图像格式为:', val_img.shape, '验证集标签格式为:', val_lb.shape)\n",
    "print('测试集图像格式为:', test_img.shape, '测试集标签格式为:', test_lb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ff0600",
   "metadata": {},
   "source": [
    "## 两种方式计算Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257f42c4",
   "metadata": {},
   "source": [
    "### 用for循环计算Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fb7ecb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_softmax(X, y, W):\n",
    "    \"\"\"\n",
    "    Inputs have dimension D=784, there are C=10 classes, and we operate on N=50000 examples.\n",
    "    \n",
    "    Inputs:\n",
    "    - W: Indexs of linear classifier, a numpy array of shape (D, C) containing weights.\n",
    "    - X: Training images, a numpy array of shape (N, D) containing a minibatch of data.\n",
    "    - y: Training labels, a numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "         that X[i] has label c, where 0 <= c < C.\n",
    "    \n",
    "    Returns a tuple of:\n",
    "    - Softmax loss as single float.\n",
    "    \"\"\"\n",
    "    \n",
    "    # set the value of delta, lamda\n",
    "    delta = 1.0\n",
    "    lamda = 1.0\n",
    "    loss = 0.0\n",
    "    num_train = X.shape[0]\n",
    "    num_class = W.shape[1]\n",
    "    \n",
    "    for i in range(num_train):\n",
    "        # scores: class x 1\n",
    "        scores = W.T.dot(X[i][:]).reshape(num_class, 1)\n",
    " \n",
    "        # get the max score\n",
    "        scores_max = np.max(scores, axis=0)\n",
    "        \n",
    "        # prob: class x 1, calculate the log probability\n",
    "        # use scores_max to limit the boundary of exp indexes\n",
    "        prob = np.exp(scores-scores_max) / np.sum(np.exp(scores-scores_max), axis=0)\n",
    "        \n",
    "        # calculate the loss\n",
    "        for j in range(num_class):\n",
    "            if j == y[i]:\n",
    "                # accumulate loss for the i-th example\n",
    "                loss -=np.log(prob[j])\n",
    "    # calculate the average data loss\n",
    "    loss = loss[0]/num_train\n",
    "    \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bf7eed",
   "metadata": {},
   "source": [
    "### 用向量化计算Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "46e3c907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_loss_softmax(X, y, W):\n",
    "    \"\"\"\n",
    "    Inputs have dimension D=784, there are C=10 classes, and we operate on N=50000 examples.\n",
    "    \n",
    "    Inputs:\n",
    "    - W: Indexs of linear classifier, a numpy array of shape (D, C) containing weights.\n",
    "    - X: Training images, a numpy array of shape (N, D) containing a minibatch of data.\n",
    "    - y: Training labels, a numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "         that X[i] has label c, where 0 <= c < C.\n",
    "    \n",
    "    Returns a tuple of:\n",
    "    - Softmax loss as single float.\n",
    "    \"\"\"\n",
    "    \n",
    "    # set the value of delta, lamda\n",
    "    delta = 1.0\n",
    "    lamda = 1.0\n",
    "    num_train = X.shape[0]\n",
    "    num_class = W.shape[1]\n",
    "    \n",
    "    # scores: class x examples\n",
    "    scores = W.T.dot(X.T).reshape(num_class, num_train)\n",
    "    \n",
    "    # scores_max: 1 x examples, get the max value from each column\n",
    "    scores_max = np.reshape(np.max(scores, axis=0), (1, num_train))\n",
    "\n",
    "    # prob: class x examples, calculate the log probability\n",
    "    # use scores_max to limit the boundary of exp indexes\n",
    "    prob = np.exp(scores-scores_max) / np.sum(np.exp(scores-scores_max), axis=0)\n",
    "\n",
    "    # set value 1 in true label positions, 0 for false labels\n",
    "    y_true = np.zeros(prob.shape)\n",
    "    y_true[y, np.arange(num_train)] = 1.0\n",
    "    # calculate the average data loss\n",
    "    loss = np.sum(-y_true*np.log(prob))/num_train\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edadef6d",
   "metadata": {},
   "source": [
    "### 比较向量化和for循环的运行时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "997e4f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for的损失值loss: 2.300610152181934\n",
      "矩阵的损失值loss: 2.300610152181918\n",
      "for用时: 2.722450017929077\n",
      "矩阵用时: 0.3175334930419922\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "X=train_img#[0:1]\n",
    "y=train_lb#[0:1]\n",
    "W=np.random.rand(784,10)*0.0001\n",
    "timea=time.time()\n",
    "print(\"for的损失值loss:\",loss_softmax(X, y, W))\n",
    "timea=time.time()-timea\n",
    "\n",
    "timeb=time.time()\n",
    "print(\"矩阵的损失值loss:\",vectorized_loss_softmax(X, y, W))\n",
    "timeb=time.time()-timeb\n",
    "\n",
    "print(\"for用时:\",timea)\n",
    "print(\"矩阵用时:\",timeb)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a7673bb-44b7-4981-b453-cba361417521",
   "metadata": {},
   "source": [
    "for的损失值loss: 2.300610152181934\n",
    "矩阵的损失值loss: 2.300610152181918\n",
    "两者的损失值相同都能取得相同的效果\n",
    "\n",
    "for用时: 2.722450017929077\n",
    "矩阵用时: 0.3175334930419922\n",
    "但是手写的for循环计算时间比直接调用numpy的库多了2.4秒\n",
    "个人认为有以下几点\n",
    "1运算时没有考虑利用cpu的空间局限性，应当按照储存的顺序访问矩阵元素\n",
    "2大量运算都可以并行化，如果采用并行化算法能够节省不少时间"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
