{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集图像格式为: (50000, 28, 28) 训练集标签格式为: (50000,) 热编码训练集标签格式为: (50000, 10)\n",
      "验证集图像格式为: (10000, 28, 28) 验证集标签格式为: (10000,) 热编码验证集标签格式为: (10000, 10)\n",
      "测试集图像格式为: (10000, 28, 28) 测试集标签格式为: (10000,) 热编码测试集标签格式为: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import gzip\n",
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"加载MNIST数据集\"\"\"\n",
    "    labels_path = os.path.join(path, f'{kind}-labels-idx1-ubyte.gz')\n",
    "    images_path = os.path.join(path, f'{kind}-images-idx3-ubyte.gz')\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        struct.unpack('>II', lbpath.read(8))\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        struct.unpack('>IIII', imgpath.read(16))\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8).reshape(len(labels),28,28)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "\n",
    "# 数据集划分\n",
    "def data_split(images, labels, ratio):\n",
    "    \n",
    "    total_len = images.shape[0]\n",
    "    offset = int(total_len * ratio)\n",
    "\n",
    "    val_img = images[:offset][:]\n",
    "    val_lb = labels[:offset]\n",
    "\n",
    "    train_img = images[offset:][:]\n",
    "    train_lb = labels[offset:]\n",
    "\n",
    "    return train_img, train_lb, val_img, val_lb    \n",
    "\n",
    "# 读取训练集和测试集数据\n",
    "[images, labels] = load_mnist('./MNIST', kind='train')\n",
    "[test_img, test_lb] = load_mnist('./MNIST',kind='test')\n",
    "train_img, train_lb, val_img, val_lb = data_split(images, labels, 1/6)\n",
    "\n",
    "# 对标签进行热编码\n",
    "one_hot_train_lb = np.eye(10)[train_lb]\n",
    "one_hot_val_lb = np.eye(10)[val_lb]\n",
    "one_hot_test_lb= np.eye(10)[test_lb]\n",
    "\n",
    "# 打印查看数据集格式\n",
    "print('训练集图像格式为:', train_img.shape, '训练集标签格式为:', train_lb.shape,'热编码训练集标签格式为:', one_hot_train_lb.shape)\n",
    "print('验证集图像格式为:', val_img.shape, '验证集标签格式为:', val_lb.shape,'热编码验证集标签格式为:', one_hot_val_lb.shape)\n",
    "print('测试集图像格式为:', test_img.shape, '测试集标签格式为:', test_lb.shape,'热编码测试集标签格式为:', one_hot_test_lb.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_426695/860925982.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_img=torch.tensor(train_img)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import v2 \n",
    "trans=v2.Compose([\n",
    "    v2.RandomResizedCrop(size=(28,28),antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomHorizontalFlip(p=0.5)\n",
    "])\n",
    "train_img=torch.tensor(train_img)\n",
    "new_X=trans(train_img)\n",
    "new_X=torch.cat((train_img,new_X),dim=0)\n",
    "new_Y=torch.tensor(one_hot_train_lb)\n",
    "new_Y=torch.cat((new_Y,new_Y),dim=0)\n",
    "train_img=new_X\n",
    "one_hot_train_lb=new_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "        self.w1=nn.Linear(28*28, 512)\n",
    "        self.w2=nn.Linear(512, 512)\n",
    "        self.w3=nn.Linear(512, 10)\n",
    "        self.drop=nn.Dropout(p=0.5)\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(len(x),-1)\n",
    "        x = self.w1 (x)\n",
    "        x = self.relu(x)\n",
    "        x= self.drop(x)\n",
    "        x = self.w2 (x)\n",
    "        x = self.relu(x)\n",
    "        self.drop(x)\n",
    "        x = self.w3 (x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork()\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in model.modules():\n",
    "    if isinstance(i,nn.Linear):\n",
    "        nn.init.xavier_uniform_(i.weight)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch:1, loss: 4.368667,  Accuracy: 79.8%\n",
      " Epoch:2, loss: 0.815133,  Accuracy: 83.2%\n",
      " Epoch:3, loss: 0.686101,  Accuracy: 85.1%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 100\n",
    "epochs = 100\n",
    "batch_num=int(train_img.shape[0]/batch_size)\n",
    "size = len(train_img)\n",
    "\n",
    "model.train()\n",
    "for t in range(epochs):\n",
    "    \n",
    "    correct=0.\n",
    "    train_mean_loss=0.\n",
    "\n",
    "    for batch in range(batch_num):\n",
    "        X=train_img[batch*batch_size:(batch+1)*batch_size,]\n",
    "        y=one_hot_train_lb[batch*batch_size:(batch+1)*batch_size,:]\n",
    "\n",
    "        X=torch.tensor(X, dtype=torch.float32)\n",
    "        y=torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X).type(torch.float)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).mean().item()\n",
    "        train_mean_loss+= loss.item()\n",
    "\n",
    "    train_mean_loss /= batch_num\n",
    "    correct /= batch_num\n",
    "    \n",
    "    print(f\" Epoch:{t+1}, loss: {train_mean_loss:>8f},  Accuracy: {(100*correct):>0.1f}%\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss, correct = 0, 0\n",
    "with torch.no_grad():\n",
    "        X=torch.tensor(test_img, dtype=torch.float32)\n",
    "        y=torch.tensor(one_hot_test_lb, dtype=torch.float32)\n",
    "        pred = model(X)\n",
    "        test_loss = np.mean(loss_fn(pred, y).item())\n",
    "        correct = (pred.argmax(1) == y.argmax(1)).type(torch.float).mean().item()\n",
    "\n",
    "print(f\"Test Accuracy: {(100*correct):>0.1f}%, Test Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Epoch:10, loss: 0.086816,  Accuracy: 97.7%\n",
    "Test Accuracy: 96.9%, Test Avg loss: 0.169128 \n",
    "对于手写数字体识别这个问题\n",
    "三层的线性网络已经可以达到足够的精确度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
