{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a296724e",
   "metadata": {},
   "source": [
    "# 实验5  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "699ecd93",
   "metadata": {},
   "source": [
    "将下面的代码补充完整"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0ad1f84",
   "metadata": {},
   "source": [
    "## 读取MNIST数据集，并将其划分为train/val/test数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "56e3880e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集图像格式为: (50000, 784) 训练集标签格式为: (50000,) 热编码训练集标签格式为: (50000, 10)\n",
      "验证集图像格式为: (10000, 784) 验证集标签格式为: (10000,) 热编码验证集标签格式为: (10000, 10)\n",
      "测试集图像格式为: (10000, 784) 测试集标签格式为: (10000,) 热编码测试集标签格式为: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"加载MNIST数据集\"\"\"\n",
    "    labels_path = os.path.join(path, f'{kind}-labels-idx1-ubyte.gz')\n",
    "    images_path = os.path.join(path, f'{kind}-images-idx3-ubyte.gz')\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        struct.unpack('>II', lbpath.read(8))\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        struct.unpack('>IIII', imgpath.read(16))\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "\n",
    "# 数据集划分\n",
    "def data_split(images, labels, ratio):\n",
    "    \n",
    "    total_len = images.shape[0]\n",
    "    offset = int(total_len * ratio)\n",
    "\n",
    "    val_img = images[:offset][:]\n",
    "    val_lb = labels[:offset]\n",
    "\n",
    "    train_img = images[offset:][:]\n",
    "    train_lb = labels[offset:]\n",
    "\n",
    "    return train_img, train_lb, val_img, val_lb    \n",
    "\n",
    "# 读取训练集和测试集数据\n",
    "[images, labels] = load_mnist('./MNIST', kind='train')\n",
    "[test_img, test_lb] = load_mnist('./MNIST',kind='test')\n",
    "train_img, train_lb, val_img, val_lb = data_split(images, labels, 1/6)\n",
    "\n",
    "# 对标签进行热编码\n",
    "one_hot_train_lb = np.eye(10)[train_lb]\n",
    "one_hot_val_lb = np.eye(10)[val_lb]\n",
    "one_hot_test_lb= np.eye(10)[test_lb]\n",
    "\n",
    "# 打印查看数据集格式\n",
    "print('训练集图像格式为:', train_img.shape, '训练集标签格式为:', train_lb.shape,'热编码训练集标签格式为:', one_hot_train_lb.shape)\n",
    "print('验证集图像格式为:', val_img.shape, '验证集标签格式为:', val_lb.shape,'热编码验证集标签格式为:', one_hot_val_lb.shape)\n",
    "print('测试集图像格式为:', test_img.shape, '测试集标签格式为:', test_lb.shape,'热编码测试集标签格式为:', one_hot_test_lb.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd299dbe",
   "metadata": {},
   "source": [
    "## 神经网络(2层线性分类器)+L2 Loss+梯度后向传播+Adam优化器"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3bde0b95",
   "metadata": {},
   "source": [
    "### 定义Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1a9465c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_opt(dx, first_momentum, second_momentum, i, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "      # momentum\n",
    "      first_momentum = beta1 * first_momentum + (1-beta1) * dx\n",
    "      # adagrad\n",
    "      second_momentum = beta2 * second_momentum + (1-beta2) * dx * dx\n",
    "      # bias correction\n",
    "      first_unbias = first_momentum / (1 - beta1 ** i)\n",
    "      second_unbias = second_momentum / (1 - beta2 ** i)\n",
    "      return first_unbias / (np.sqrt(second_unbias) + eps),first_momentum,second_momentum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95efb4ad",
   "metadata": {},
   "source": [
    "### 题目1：Train network without bias (Adam) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8693355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型为两层线性分类器\n",
    "#隐藏层神经元数量64，使用sigmoid激活函数，忽略偏置b1\n",
    "#输出层没有使用激活函数,忽略偏置b2\n",
    "#使用L2损失函数\n",
    "def train1(x, y, epoch):\n",
    "    \"\"\"\n",
    "    Inputs have dimension D=784, there are C=10 classes, and we operate on N=1000 examples.\n",
    "    \n",
    "    Inputs:\n",
    "    - x: Training images, a numpy array of shape (N, D) containing a minibatch of data.\n",
    "    - y: Training labels, a numpy array of shape (N,C) containing training labels; y[c] = 1 means\n",
    "          that X[i] has label c, where 0 <= c < C.\n",
    "    - epoch: Training iterations, an integer.\n",
    "    \n",
    "    Returns:\n",
    "    - 训练得到的参数\n",
    "    \"\"\"\n",
    "    \n",
    "    # set the hyperparameters\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    # Initialize first and second momentums in Adam\n",
    "    mW1, vW1 = 0, 0\n",
    "    mW2, vW2 = 0, 0\n",
    "\n",
    "    # initialize W1 (D,64) and w2(64,C) \n",
    "    w1 = np.random.randn(x.shape[1], 64) * 0.0001\n",
    "    w2 = np.random.randn(64,y.shape[1]) * 0.0001\n",
    "\n",
    "\n",
    "    for t in range (1,epoch+1):\n",
    "      h = 1 / (1 + np.exp(-(x.dot (w1)) ) )  #(N,64)\n",
    "      y_pred = h.dot(w2)      #(N,C)\n",
    "      loss = np.square(y_pred - y).mean()\n",
    "      scores = np.argmax(y_pred, axis=1)\n",
    "      lb     = np.argmax(y,      axis=1)\n",
    "      accuracy = np.mean(scores == lb) * 100\n",
    "\n",
    "      # compute the numeric gradient\n",
    "      grad_y_pred = (y_pred-y)*2 #Nx10\n",
    "      grad_w2 = h.T.dot(grad_y_pred) #64x10\n",
    "      grad_h = grad_y_pred.dot(w2.T) #50000*64\n",
    "      grad_sig=h*(1-h)*grad_h\n",
    "      grad_w1 = x.T.dot(grad_sig)\n",
    "\n",
    "\n",
    "      # update W and b with Adam\n",
    "      grad_w2, mW2, vW2 = adam_opt(grad_w2, mW2, vW2, t)\n",
    "      grad_w1, mW1, vW1 = adam_opt(grad_w1, mW1, vW1, t)\n",
    "\n",
    "      w2 -= learning_rate *grad_w2\n",
    "      w1 -= learning_rate *grad_w1\n",
    "\n",
    "        # print the result\n",
    "      print(\"Epoch: %d  Loss: %.3f  Acc: %.3f%%\" % (t, loss, accuracy))\n",
    "    \n",
    "    return w1, w2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "496a39e5",
   "metadata": {},
   "source": [
    "#### 在训练集上进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bab40dd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Loss: 0.100  Acc: 7.288%\n",
      "Epoch: 2  Loss: 0.095  Acc: 29.914%\n",
      "Epoch: 3  Loss: 0.091  Acc: 30.122%\n",
      "Epoch: 4  Loss: 0.090  Acc: 12.548%\n",
      "Epoch: 5  Loss: 0.091  Acc: 11.230%\n",
      "Epoch: 6  Loss: 0.092  Acc: 11.230%\n",
      "Epoch: 7  Loss: 0.091  Acc: 11.230%\n",
      "Epoch: 8  Loss: 0.091  Acc: 11.230%\n",
      "Epoch: 9  Loss: 0.090  Acc: 11.230%\n",
      "Epoch: 10  Loss: 0.090  Acc: 11.230%\n",
      "Epoch: 11  Loss: 0.089  Acc: 11.230%\n",
      "Epoch: 12  Loss: 0.090  Acc: 11.230%\n",
      "Epoch: 13  Loss: 0.089  Acc: 11.230%\n",
      "Epoch: 14  Loss: 0.089  Acc: 23.182%\n",
      "Epoch: 15  Loss: 0.089  Acc: 25.944%\n",
      "Epoch: 16  Loss: 0.089  Acc: 43.162%\n",
      "Epoch: 17  Loss: 0.088  Acc: 45.474%\n",
      "Epoch: 18  Loss: 0.088  Acc: 47.610%\n",
      "Epoch: 19  Loss: 0.088  Acc: 47.344%\n",
      "Epoch: 20  Loss: 0.087  Acc: 46.210%\n",
      "Epoch: 21  Loss: 0.087  Acc: 45.916%\n",
      "Epoch: 22  Loss: 0.087  Acc: 45.906%\n",
      "Epoch: 23  Loss: 0.087  Acc: 48.990%\n",
      "Epoch: 24  Loss: 0.086  Acc: 50.148%\n",
      "Epoch: 25  Loss: 0.086  Acc: 51.266%\n",
      "Epoch: 26  Loss: 0.085  Acc: 51.886%\n",
      "Epoch: 27  Loss: 0.085  Acc: 52.282%\n",
      "Epoch: 28  Loss: 0.085  Acc: 52.862%\n",
      "Epoch: 29  Loss: 0.085  Acc: 53.476%\n",
      "Epoch: 30  Loss: 0.084  Acc: 57.468%\n",
      "Epoch: 31  Loss: 0.084  Acc: 57.962%\n",
      "Epoch: 32  Loss: 0.084  Acc: 58.366%\n",
      "Epoch: 33  Loss: 0.083  Acc: 58.652%\n",
      "Epoch: 34  Loss: 0.083  Acc: 58.988%\n",
      "Epoch: 35  Loss: 0.083  Acc: 59.164%\n",
      "Epoch: 36  Loss: 0.082  Acc: 59.062%\n",
      "Epoch: 37  Loss: 0.082  Acc: 55.214%\n",
      "Epoch: 38  Loss: 0.082  Acc: 55.072%\n",
      "Epoch: 39  Loss: 0.082  Acc: 55.146%\n",
      "Epoch: 40  Loss: 0.081  Acc: 59.166%\n",
      "Epoch: 41  Loss: 0.081  Acc: 59.434%\n",
      "Epoch: 42  Loss: 0.081  Acc: 59.734%\n",
      "Epoch: 43  Loss: 0.080  Acc: 59.910%\n",
      "Epoch: 44  Loss: 0.080  Acc: 59.980%\n",
      "Epoch: 45  Loss: 0.080  Acc: 60.024%\n",
      "Epoch: 46  Loss: 0.080  Acc: 60.118%\n",
      "Epoch: 47  Loss: 0.079  Acc: 60.162%\n",
      "Epoch: 48  Loss: 0.079  Acc: 60.244%\n",
      "Epoch: 49  Loss: 0.079  Acc: 60.304%\n",
      "Epoch: 50  Loss: 0.079  Acc: 60.382%\n",
      "Epoch: 51  Loss: 0.078  Acc: 60.442%\n",
      "Epoch: 52  Loss: 0.078  Acc: 60.584%\n",
      "Epoch: 53  Loss: 0.078  Acc: 60.656%\n",
      "Epoch: 54  Loss: 0.078  Acc: 60.658%\n",
      "Epoch: 55  Loss: 0.077  Acc: 60.690%\n",
      "Epoch: 56  Loss: 0.077  Acc: 60.814%\n",
      "Epoch: 57  Loss: 0.077  Acc: 60.854%\n",
      "Epoch: 58  Loss: 0.077  Acc: 60.944%\n",
      "Epoch: 59  Loss: 0.076  Acc: 61.222%\n",
      "Epoch: 60  Loss: 0.076  Acc: 66.796%\n",
      "Epoch: 61  Loss: 0.076  Acc: 67.160%\n",
      "Epoch: 62  Loss: 0.076  Acc: 67.652%\n",
      "Epoch: 63  Loss: 0.075  Acc: 67.932%\n",
      "Epoch: 64  Loss: 0.075  Acc: 68.234%\n",
      "Epoch: 65  Loss: 0.075  Acc: 68.468%\n",
      "Epoch: 66  Loss: 0.075  Acc: 68.774%\n",
      "Epoch: 67  Loss: 0.075  Acc: 68.952%\n",
      "Epoch: 68  Loss: 0.074  Acc: 69.164%\n",
      "Epoch: 69  Loss: 0.074  Acc: 69.416%\n",
      "Epoch: 70  Loss: 0.074  Acc: 69.564%\n",
      "Epoch: 71  Loss: 0.074  Acc: 70.054%\n",
      "Epoch: 72  Loss: 0.074  Acc: 70.136%\n",
      "Epoch: 73  Loss: 0.073  Acc: 70.300%\n",
      "Epoch: 74  Loss: 0.073  Acc: 70.678%\n",
      "Epoch: 75  Loss: 0.073  Acc: 70.978%\n",
      "Epoch: 76  Loss: 0.073  Acc: 71.284%\n",
      "Epoch: 77  Loss: 0.072  Acc: 71.500%\n",
      "Epoch: 78  Loss: 0.072  Acc: 71.824%\n",
      "Epoch: 79  Loss: 0.072  Acc: 72.082%\n",
      "Epoch: 80  Loss: 0.072  Acc: 72.406%\n",
      "Epoch: 81  Loss: 0.072  Acc: 72.680%\n",
      "Epoch: 82  Loss: 0.072  Acc: 73.094%\n",
      "Epoch: 83  Loss: 0.071  Acc: 73.326%\n",
      "Epoch: 84  Loss: 0.071  Acc: 73.706%\n",
      "Epoch: 85  Loss: 0.071  Acc: 73.748%\n",
      "Epoch: 86  Loss: 0.071  Acc: 73.998%\n",
      "Epoch: 87  Loss: 0.071  Acc: 74.180%\n",
      "Epoch: 88  Loss: 0.070  Acc: 74.246%\n",
      "Epoch: 89  Loss: 0.070  Acc: 74.564%\n",
      "Epoch: 90  Loss: 0.070  Acc: 74.486%\n",
      "Epoch: 91  Loss: 0.070  Acc: 74.782%\n",
      "Epoch: 92  Loss: 0.070  Acc: 74.784%\n",
      "Epoch: 93  Loss: 0.070  Acc: 74.568%\n",
      "Epoch: 94  Loss: 0.069  Acc: 74.978%\n",
      "Epoch: 95  Loss: 0.069  Acc: 75.142%\n",
      "Epoch: 96  Loss: 0.069  Acc: 74.636%\n",
      "Epoch: 97  Loss: 0.069  Acc: 75.204%\n",
      "Epoch: 98  Loss: 0.069  Acc: 75.058%\n",
      "Epoch: 99  Loss: 0.069  Acc: 75.282%\n",
      "Epoch: 100  Loss: 0.069  Acc: 74.966%\n",
      "Epoch: 101  Loss: 0.068  Acc: 75.180%\n",
      "Epoch: 102  Loss: 0.068  Acc: 75.280%\n",
      "Epoch: 103  Loss: 0.068  Acc: 75.532%\n",
      "Epoch: 104  Loss: 0.068  Acc: 75.282%\n",
      "Epoch: 105  Loss: 0.068  Acc: 75.394%\n",
      "Epoch: 106  Loss: 0.068  Acc: 75.490%\n",
      "Epoch: 107  Loss: 0.067  Acc: 75.600%\n",
      "Epoch: 108  Loss: 0.067  Acc: 75.710%\n",
      "Epoch: 109  Loss: 0.067  Acc: 75.446%\n",
      "Epoch: 110  Loss: 0.067  Acc: 75.588%\n",
      "Epoch: 111  Loss: 0.067  Acc: 75.724%\n",
      "Epoch: 112  Loss: 0.067  Acc: 75.798%\n",
      "Epoch: 113  Loss: 0.067  Acc: 75.548%\n",
      "Epoch: 114  Loss: 0.067  Acc: 75.796%\n",
      "Epoch: 115  Loss: 0.066  Acc: 75.976%\n",
      "Epoch: 116  Loss: 0.066  Acc: 75.800%\n",
      "Epoch: 117  Loss: 0.066  Acc: 76.306%\n",
      "Epoch: 118  Loss: 0.066  Acc: 76.116%\n",
      "Epoch: 119  Loss: 0.066  Acc: 76.334%\n",
      "Epoch: 120  Loss: 0.066  Acc: 76.318%\n",
      "Epoch: 121  Loss: 0.066  Acc: 76.314%\n",
      "Epoch: 122  Loss: 0.066  Acc: 76.470%\n",
      "Epoch: 123  Loss: 0.066  Acc: 76.506%\n",
      "Epoch: 124  Loss: 0.065  Acc: 76.364%\n",
      "Epoch: 125  Loss: 0.065  Acc: 76.508%\n",
      "Epoch: 126  Loss: 0.065  Acc: 76.228%\n",
      "Epoch: 127  Loss: 0.065  Acc: 77.416%\n",
      "Epoch: 128  Loss: 0.065  Acc: 77.398%\n",
      "Epoch: 129  Loss: 0.065  Acc: 76.998%\n",
      "Epoch: 130  Loss: 0.065  Acc: 77.576%\n",
      "Epoch: 131  Loss: 0.065  Acc: 77.608%\n",
      "Epoch: 132  Loss: 0.065  Acc: 77.154%\n",
      "Epoch: 133  Loss: 0.065  Acc: 77.796%\n",
      "Epoch: 134  Loss: 0.065  Acc: 77.570%\n",
      "Epoch: 135  Loss: 0.064  Acc: 77.862%\n",
      "Epoch: 136  Loss: 0.064  Acc: 77.912%\n",
      "Epoch: 137  Loss: 0.064  Acc: 77.786%\n",
      "Epoch: 138  Loss: 0.064  Acc: 77.948%\n",
      "Epoch: 139  Loss: 0.064  Acc: 77.800%\n",
      "Epoch: 140  Loss: 0.064  Acc: 78.124%\n",
      "Epoch: 141  Loss: 0.064  Acc: 78.074%\n",
      "Epoch: 142  Loss: 0.064  Acc: 77.958%\n",
      "Epoch: 143  Loss: 0.064  Acc: 77.984%\n",
      "Epoch: 144  Loss: 0.064  Acc: 77.888%\n",
      "Epoch: 145  Loss: 0.063  Acc: 78.082%\n",
      "Epoch: 146  Loss: 0.063  Acc: 78.040%\n",
      "Epoch: 147  Loss: 0.063  Acc: 78.106%\n",
      "Epoch: 148  Loss: 0.063  Acc: 78.174%\n",
      "Epoch: 149  Loss: 0.063  Acc: 78.016%\n",
      "Epoch: 150  Loss: 0.063  Acc: 78.028%\n",
      "Epoch: 151  Loss: 0.063  Acc: 78.170%\n",
      "Epoch: 152  Loss: 0.063  Acc: 78.342%\n",
      "Epoch: 153  Loss: 0.063  Acc: 78.242%\n",
      "Epoch: 154  Loss: 0.063  Acc: 78.268%\n",
      "Epoch: 155  Loss: 0.063  Acc: 78.312%\n",
      "Epoch: 156  Loss: 0.063  Acc: 78.098%\n",
      "Epoch: 157  Loss: 0.063  Acc: 78.128%\n",
      "Epoch: 158  Loss: 0.062  Acc: 78.208%\n",
      "Epoch: 159  Loss: 0.062  Acc: 78.306%\n",
      "Epoch: 160  Loss: 0.062  Acc: 78.342%\n",
      "Epoch: 161  Loss: 0.062  Acc: 78.292%\n",
      "Epoch: 162  Loss: 0.062  Acc: 78.356%\n",
      "Epoch: 163  Loss: 0.062  Acc: 78.216%\n",
      "Epoch: 164  Loss: 0.062  Acc: 78.304%\n",
      "Epoch: 165  Loss: 0.062  Acc: 78.462%\n",
      "Epoch: 166  Loss: 0.062  Acc: 78.486%\n",
      "Epoch: 167  Loss: 0.062  Acc: 78.428%\n",
      "Epoch: 168  Loss: 0.062  Acc: 78.400%\n",
      "Epoch: 169  Loss: 0.062  Acc: 78.340%\n",
      "Epoch: 170  Loss: 0.062  Acc: 78.188%\n",
      "Epoch: 171  Loss: 0.062  Acc: 78.176%\n",
      "Epoch: 172  Loss: 0.062  Acc: 78.420%\n",
      "Epoch: 173  Loss: 0.061  Acc: 78.596%\n",
      "Epoch: 174  Loss: 0.061  Acc: 78.506%\n",
      "Epoch: 175  Loss: 0.061  Acc: 78.398%\n",
      "Epoch: 176  Loss: 0.061  Acc: 78.330%\n",
      "Epoch: 177  Loss: 0.061  Acc: 78.136%\n",
      "Epoch: 178  Loss: 0.061  Acc: 78.144%\n",
      "Epoch: 179  Loss: 0.061  Acc: 78.602%\n",
      "Epoch: 180  Loss: 0.061  Acc: 78.416%\n",
      "Epoch: 181  Loss: 0.061  Acc: 78.316%\n",
      "Epoch: 182  Loss: 0.061  Acc: 78.434%\n",
      "Epoch: 183  Loss: 0.061  Acc: 78.482%\n",
      "Epoch: 184  Loss: 0.061  Acc: 78.630%\n",
      "Epoch: 185  Loss: 0.061  Acc: 78.600%\n",
      "Epoch: 186  Loss: 0.061  Acc: 78.588%\n",
      "Epoch: 187  Loss: 0.061  Acc: 78.584%\n",
      "Epoch: 188  Loss: 0.061  Acc: 78.452%\n",
      "Epoch: 189  Loss: 0.061  Acc: 78.678%\n",
      "Epoch: 190  Loss: 0.061  Acc: 78.620%\n",
      "Epoch: 191  Loss: 0.061  Acc: 78.698%\n",
      "Epoch: 192  Loss: 0.061  Acc: 78.324%\n",
      "Epoch: 193  Loss: 0.060  Acc: 78.774%\n",
      "Epoch: 194  Loss: 0.060  Acc: 78.744%\n",
      "Epoch: 195  Loss: 0.060  Acc: 78.746%\n",
      "Epoch: 196  Loss: 0.060  Acc: 78.788%\n",
      "Epoch: 197  Loss: 0.060  Acc: 78.472%\n",
      "Epoch: 198  Loss: 0.060  Acc: 78.788%\n",
      "Epoch: 199  Loss: 0.060  Acc: 78.260%\n",
      "Epoch: 200  Loss: 0.060  Acc: 78.742%\n",
      "Epoch: 201  Loss: 0.060  Acc: 78.652%\n",
      "Epoch: 202  Loss: 0.060  Acc: 78.202%\n",
      "Epoch: 203  Loss: 0.060  Acc: 78.820%\n",
      "Epoch: 204  Loss: 0.060  Acc: 78.802%\n",
      "Epoch: 205  Loss: 0.060  Acc: 78.518%\n",
      "Epoch: 206  Loss: 0.060  Acc: 78.772%\n",
      "Epoch: 207  Loss: 0.060  Acc: 78.454%\n",
      "Epoch: 208  Loss: 0.060  Acc: 78.696%\n",
      "Epoch: 209  Loss: 0.060  Acc: 78.738%\n",
      "Epoch: 210  Loss: 0.060  Acc: 78.022%\n",
      "Epoch: 211  Loss: 0.060  Acc: 78.804%\n",
      "Epoch: 212  Loss: 0.060  Acc: 78.012%\n",
      "Epoch: 213  Loss: 0.059  Acc: 78.878%\n",
      "Epoch: 214  Loss: 0.060  Acc: 78.066%\n",
      "Epoch: 215  Loss: 0.059  Acc: 78.976%\n",
      "Epoch: 216  Loss: 0.060  Acc: 78.652%\n",
      "Epoch: 217  Loss: 0.059  Acc: 78.600%\n",
      "Epoch: 218  Loss: 0.059  Acc: 78.802%\n",
      "Epoch: 219  Loss: 0.059  Acc: 78.642%\n",
      "Epoch: 220  Loss: 0.059  Acc: 78.912%\n",
      "Epoch: 221  Loss: 0.059  Acc: 78.710%\n",
      "Epoch: 222  Loss: 0.059  Acc: 78.774%\n",
      "Epoch: 223  Loss: 0.059  Acc: 78.930%\n",
      "Epoch: 224  Loss: 0.059  Acc: 78.854%\n",
      "Epoch: 225  Loss: 0.059  Acc: 78.812%\n",
      "Epoch: 226  Loss: 0.059  Acc: 78.820%\n",
      "Epoch: 227  Loss: 0.059  Acc: 78.982%\n",
      "Epoch: 228  Loss: 0.059  Acc: 78.886%\n",
      "Epoch: 229  Loss: 0.059  Acc: 78.964%\n",
      "Epoch: 230  Loss: 0.059  Acc: 78.916%\n",
      "Epoch: 231  Loss: 0.059  Acc: 78.896%\n",
      "Epoch: 232  Loss: 0.059  Acc: 78.978%\n",
      "Epoch: 233  Loss: 0.059  Acc: 79.056%\n",
      "Epoch: 234  Loss: 0.059  Acc: 78.974%\n",
      "Epoch: 235  Loss: 0.059  Acc: 79.150%\n",
      "Epoch: 236  Loss: 0.059  Acc: 79.044%\n",
      "Epoch: 237  Loss: 0.059  Acc: 78.956%\n",
      "Epoch: 238  Loss: 0.059  Acc: 78.882%\n",
      "Epoch: 239  Loss: 0.059  Acc: 78.630%\n",
      "Epoch: 240  Loss: 0.058  Acc: 79.108%\n",
      "Epoch: 241  Loss: 0.059  Acc: 78.908%\n",
      "Epoch: 242  Loss: 0.058  Acc: 78.882%\n",
      "Epoch: 243  Loss: 0.058  Acc: 78.950%\n",
      "Epoch: 244  Loss: 0.059  Acc: 78.814%\n",
      "Epoch: 245  Loss: 0.058  Acc: 78.862%\n",
      "Epoch: 246  Loss: 0.058  Acc: 78.586%\n",
      "Epoch: 247  Loss: 0.058  Acc: 79.030%\n",
      "Epoch: 248  Loss: 0.058  Acc: 78.938%\n",
      "Epoch: 249  Loss: 0.058  Acc: 79.202%\n",
      "Epoch: 250  Loss: 0.058  Acc: 78.662%\n",
      "Epoch: 251  Loss: 0.058  Acc: 78.816%\n",
      "Epoch: 252  Loss: 0.058  Acc: 78.896%\n",
      "Epoch: 253  Loss: 0.058  Acc: 79.082%\n",
      "Epoch: 254  Loss: 0.058  Acc: 78.970%\n",
      "Epoch: 255  Loss: 0.058  Acc: 78.884%\n",
      "Epoch: 256  Loss: 0.058  Acc: 78.942%\n",
      "Epoch: 257  Loss: 0.058  Acc: 79.024%\n",
      "Epoch: 258  Loss: 0.058  Acc: 79.086%\n",
      "Epoch: 259  Loss: 0.058  Acc: 78.982%\n",
      "Epoch: 260  Loss: 0.058  Acc: 79.070%\n",
      "Epoch: 261  Loss: 0.058  Acc: 79.036%\n",
      "Epoch: 262  Loss: 0.058  Acc: 78.862%\n",
      "Epoch: 263  Loss: 0.058  Acc: 78.640%\n",
      "Epoch: 264  Loss: 0.058  Acc: 78.902%\n",
      "Epoch: 265  Loss: 0.058  Acc: 79.082%\n",
      "Epoch: 266  Loss: 0.058  Acc: 79.080%\n",
      "Epoch: 267  Loss: 0.058  Acc: 78.882%\n",
      "Epoch: 268  Loss: 0.058  Acc: 78.774%\n",
      "Epoch: 269  Loss: 0.058  Acc: 79.150%\n",
      "Epoch: 270  Loss: 0.058  Acc: 79.024%\n",
      "Epoch: 271  Loss: 0.058  Acc: 79.070%\n",
      "Epoch: 272  Loss: 0.058  Acc: 78.858%\n",
      "Epoch: 273  Loss: 0.058  Acc: 79.112%\n",
      "Epoch: 274  Loss: 0.057  Acc: 79.202%\n",
      "Epoch: 275  Loss: 0.057  Acc: 78.996%\n",
      "Epoch: 276  Loss: 0.057  Acc: 79.132%\n",
      "Epoch: 277  Loss: 0.057  Acc: 78.844%\n",
      "Epoch: 278  Loss: 0.057  Acc: 78.974%\n",
      "Epoch: 279  Loss: 0.057  Acc: 79.068%\n",
      "Epoch: 280  Loss: 0.057  Acc: 79.134%\n",
      "Epoch: 281  Loss: 0.057  Acc: 79.228%\n",
      "Epoch: 282  Loss: 0.057  Acc: 79.054%\n",
      "Epoch: 283  Loss: 0.057  Acc: 79.084%\n",
      "Epoch: 284  Loss: 0.057  Acc: 78.922%\n",
      "Epoch: 285  Loss: 0.057  Acc: 79.036%\n",
      "Epoch: 286  Loss: 0.057  Acc: 79.040%\n",
      "Epoch: 287  Loss: 0.057  Acc: 79.216%\n",
      "Epoch: 288  Loss: 0.057  Acc: 79.188%\n",
      "Epoch: 289  Loss: 0.057  Acc: 79.046%\n",
      "Epoch: 290  Loss: 0.057  Acc: 79.138%\n",
      "Epoch: 291  Loss: 0.057  Acc: 78.860%\n",
      "Epoch: 292  Loss: 0.057  Acc: 79.090%\n",
      "Epoch: 293  Loss: 0.057  Acc: 79.028%\n",
      "Epoch: 294  Loss: 0.057  Acc: 79.240%\n",
      "Epoch: 295  Loss: 0.057  Acc: 79.232%\n",
      "Epoch: 296  Loss: 0.057  Acc: 79.120%\n",
      "Epoch: 297  Loss: 0.057  Acc: 79.206%\n",
      "Epoch: 298  Loss: 0.057  Acc: 78.900%\n",
      "Epoch: 299  Loss: 0.057  Acc: 79.098%\n",
      "Epoch: 300  Loss: 0.057  Acc: 79.134%\n",
      "Epoch: 301  Loss: 0.057  Acc: 79.230%\n",
      "Epoch: 302  Loss: 0.057  Acc: 79.256%\n",
      "Epoch: 303  Loss: 0.057  Acc: 79.150%\n",
      "Epoch: 304  Loss: 0.057  Acc: 79.204%\n",
      "Epoch: 305  Loss: 0.057  Acc: 79.014%\n",
      "Epoch: 306  Loss: 0.057  Acc: 79.132%\n",
      "Epoch: 307  Loss: 0.057  Acc: 79.106%\n",
      "Epoch: 308  Loss: 0.057  Acc: 79.264%\n",
      "Epoch: 309  Loss: 0.057  Acc: 79.232%\n",
      "Epoch: 310  Loss: 0.057  Acc: 79.158%\n",
      "Epoch: 311  Loss: 0.057  Acc: 79.244%\n",
      "Epoch: 312  Loss: 0.057  Acc: 79.142%\n",
      "Epoch: 313  Loss: 0.057  Acc: 79.184%\n",
      "Epoch: 314  Loss: 0.057  Acc: 79.232%\n",
      "Epoch: 315  Loss: 0.057  Acc: 79.186%\n",
      "Epoch: 316  Loss: 0.056  Acc: 79.260%\n",
      "Epoch: 317  Loss: 0.056  Acc: 79.378%\n",
      "Epoch: 318  Loss: 0.056  Acc: 79.308%\n",
      "Epoch: 319  Loss: 0.056  Acc: 79.316%\n",
      "Epoch: 320  Loss: 0.056  Acc: 79.368%\n",
      "Epoch: 321  Loss: 0.056  Acc: 79.362%\n",
      "Epoch: 322  Loss: 0.056  Acc: 79.340%\n",
      "Epoch: 323  Loss: 0.056  Acc: 79.162%\n",
      "Epoch: 324  Loss: 0.056  Acc: 79.302%\n",
      "Epoch: 325  Loss: 0.056  Acc: 79.278%\n",
      "Epoch: 326  Loss: 0.056  Acc: 79.324%\n",
      "Epoch: 327  Loss: 0.056  Acc: 79.442%\n",
      "Epoch: 328  Loss: 0.056  Acc: 79.434%\n",
      "Epoch: 329  Loss: 0.056  Acc: 79.360%\n",
      "Epoch: 330  Loss: 0.056  Acc: 78.956%\n",
      "Epoch: 331  Loss: 0.056  Acc: 79.252%\n",
      "Epoch: 332  Loss: 0.056  Acc: 79.040%\n",
      "Epoch: 333  Loss: 0.056  Acc: 79.352%\n",
      "Epoch: 334  Loss: 0.056  Acc: 79.438%\n",
      "Epoch: 335  Loss: 0.056  Acc: 79.382%\n",
      "Epoch: 336  Loss: 0.056  Acc: 79.392%\n",
      "Epoch: 337  Loss: 0.056  Acc: 79.060%\n",
      "Epoch: 338  Loss: 0.056  Acc: 79.324%\n",
      "Epoch: 339  Loss: 0.056  Acc: 79.052%\n",
      "Epoch: 340  Loss: 0.056  Acc: 79.282%\n",
      "Epoch: 341  Loss: 0.056  Acc: 79.284%\n",
      "Epoch: 342  Loss: 0.056  Acc: 79.174%\n",
      "Epoch: 343  Loss: 0.056  Acc: 79.346%\n",
      "Epoch: 344  Loss: 0.056  Acc: 78.976%\n",
      "Epoch: 345  Loss: 0.056  Acc: 79.332%\n",
      "Epoch: 346  Loss: 0.056  Acc: 78.980%\n",
      "Epoch: 347  Loss: 0.056  Acc: 79.170%\n",
      "Epoch: 348  Loss: 0.056  Acc: 79.000%\n",
      "Epoch: 349  Loss: 0.056  Acc: 79.218%\n",
      "Epoch: 350  Loss: 0.056  Acc: 79.076%\n",
      "Epoch: 351  Loss: 0.056  Acc: 78.878%\n",
      "Epoch: 352  Loss: 0.056  Acc: 79.254%\n",
      "Epoch: 353  Loss: 0.056  Acc: 78.884%\n",
      "Epoch: 354  Loss: 0.056  Acc: 79.250%\n",
      "Epoch: 355  Loss: 0.056  Acc: 79.298%\n",
      "Epoch: 356  Loss: 0.056  Acc: 79.248%\n",
      "Epoch: 357  Loss: 0.056  Acc: 79.304%\n",
      "Epoch: 358  Loss: 0.056  Acc: 78.816%\n",
      "Epoch: 359  Loss: 0.056  Acc: 79.178%\n",
      "Epoch: 360  Loss: 0.056  Acc: 79.184%\n",
      "Epoch: 361  Loss: 0.055  Acc: 79.352%\n",
      "Epoch: 362  Loss: 0.055  Acc: 79.414%\n",
      "Epoch: 363  Loss: 0.055  Acc: 79.292%\n",
      "Epoch: 364  Loss: 0.055  Acc: 79.476%\n",
      "Epoch: 365  Loss: 0.055  Acc: 79.298%\n",
      "Epoch: 366  Loss: 0.055  Acc: 79.538%\n",
      "Epoch: 367  Loss: 0.055  Acc: 79.430%\n",
      "Epoch: 368  Loss: 0.055  Acc: 79.376%\n",
      "Epoch: 369  Loss: 0.055  Acc: 79.368%\n",
      "Epoch: 370  Loss: 0.055  Acc: 79.324%\n",
      "Epoch: 371  Loss: 0.055  Acc: 79.426%\n",
      "Epoch: 372  Loss: 0.055  Acc: 79.302%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_153644/2380981864.py:32: RuntimeWarning: overflow encountered in exp\n",
      "  h = 1 / (1 + np.exp(-(x.dot (w1)) ) )  #(N,64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 373  Loss: 0.054  Acc: 79.488%\n",
      "Epoch: 374  Loss: 0.054  Acc: 79.426%\n",
      "Epoch: 375  Loss: 0.054  Acc: 79.526%\n",
      "Epoch: 376  Loss: 0.054  Acc: 79.464%\n",
      "Epoch: 377  Loss: 0.054  Acc: 79.440%\n",
      "Epoch: 378  Loss: 0.054  Acc: 79.582%\n",
      "Epoch: 379  Loss: 0.053  Acc: 79.474%\n",
      "Epoch: 380  Loss: 0.053  Acc: 79.606%\n",
      "Epoch: 381  Loss: 0.053  Acc: 79.578%\n",
      "Epoch: 382  Loss: 0.053  Acc: 79.560%\n",
      "Epoch: 383  Loss: 0.053  Acc: 79.356%\n",
      "Epoch: 384  Loss: 0.053  Acc: 79.486%\n",
      "Epoch: 385  Loss: 0.052  Acc: 79.696%\n",
      "Epoch: 386  Loss: 0.052  Acc: 79.584%\n",
      "Epoch: 387  Loss: 0.052  Acc: 79.758%\n",
      "Epoch: 388  Loss: 0.052  Acc: 79.662%\n",
      "Epoch: 389  Loss: 0.052  Acc: 81.118%\n",
      "Epoch: 390  Loss: 0.052  Acc: 80.958%\n",
      "Epoch: 391  Loss: 0.052  Acc: 80.840%\n",
      "Epoch: 392  Loss: 0.051  Acc: 81.044%\n",
      "Epoch: 393  Loss: 0.051  Acc: 80.998%\n",
      "Epoch: 394  Loss: 0.051  Acc: 81.256%\n",
      "Epoch: 395  Loss: 0.051  Acc: 80.842%\n",
      "Epoch: 396  Loss: 0.051  Acc: 81.000%\n",
      "Epoch: 397  Loss: 0.051  Acc: 81.658%\n",
      "Epoch: 398  Loss: 0.051  Acc: 81.674%\n",
      "Epoch: 399  Loss: 0.051  Acc: 81.538%\n",
      "Epoch: 400  Loss: 0.051  Acc: 81.856%\n",
      "Epoch: 401  Loss: 0.051  Acc: 81.706%\n",
      "Epoch: 402  Loss: 0.050  Acc: 82.152%\n",
      "Epoch: 403  Loss: 0.050  Acc: 82.518%\n",
      "Epoch: 404  Loss: 0.050  Acc: 82.182%\n",
      "Epoch: 405  Loss: 0.050  Acc: 82.574%\n",
      "Epoch: 406  Loss: 0.050  Acc: 82.302%\n",
      "Epoch: 407  Loss: 0.050  Acc: 82.594%\n",
      "Epoch: 408  Loss: 0.050  Acc: 82.908%\n",
      "Epoch: 409  Loss: 0.050  Acc: 82.582%\n",
      "Epoch: 410  Loss: 0.050  Acc: 82.960%\n",
      "Epoch: 411  Loss: 0.050  Acc: 82.762%\n",
      "Epoch: 412  Loss: 0.050  Acc: 82.968%\n",
      "Epoch: 413  Loss: 0.049  Acc: 83.036%\n",
      "Epoch: 414  Loss: 0.050  Acc: 82.982%\n",
      "Epoch: 415  Loss: 0.049  Acc: 83.284%\n",
      "Epoch: 416  Loss: 0.049  Acc: 83.128%\n",
      "Epoch: 417  Loss: 0.049  Acc: 83.328%\n",
      "Epoch: 418  Loss: 0.049  Acc: 83.238%\n",
      "Epoch: 419  Loss: 0.049  Acc: 83.562%\n",
      "Epoch: 420  Loss: 0.049  Acc: 83.548%\n",
      "Epoch: 421  Loss: 0.049  Acc: 83.620%\n",
      "Epoch: 422  Loss: 0.049  Acc: 83.776%\n",
      "Epoch: 423  Loss: 0.049  Acc: 83.674%\n",
      "Epoch: 424  Loss: 0.049  Acc: 83.960%\n",
      "Epoch: 425  Loss: 0.049  Acc: 83.886%\n",
      "Epoch: 426  Loss: 0.049  Acc: 83.964%\n",
      "Epoch: 427  Loss: 0.049  Acc: 83.906%\n",
      "Epoch: 428  Loss: 0.049  Acc: 84.050%\n",
      "Epoch: 429  Loss: 0.049  Acc: 84.132%\n",
      "Epoch: 430  Loss: 0.049  Acc: 84.176%\n",
      "Epoch: 431  Loss: 0.048  Acc: 84.356%\n",
      "Epoch: 432  Loss: 0.048  Acc: 84.428%\n",
      "Epoch: 433  Loss: 0.048  Acc: 84.424%\n",
      "Epoch: 434  Loss: 0.048  Acc: 84.346%\n",
      "Epoch: 435  Loss: 0.048  Acc: 84.506%\n",
      "Epoch: 436  Loss: 0.048  Acc: 84.614%\n",
      "Epoch: 437  Loss: 0.048  Acc: 84.770%\n",
      "Epoch: 438  Loss: 0.048  Acc: 84.878%\n",
      "Epoch: 439  Loss: 0.048  Acc: 84.878%\n",
      "Epoch: 440  Loss: 0.048  Acc: 84.880%\n",
      "Epoch: 441  Loss: 0.048  Acc: 84.770%\n",
      "Epoch: 442  Loss: 0.048  Acc: 84.872%\n",
      "Epoch: 443  Loss: 0.048  Acc: 84.906%\n",
      "Epoch: 444  Loss: 0.048  Acc: 85.022%\n",
      "Epoch: 445  Loss: 0.048  Acc: 85.214%\n",
      "Epoch: 446  Loss: 0.048  Acc: 85.044%\n",
      "Epoch: 447  Loss: 0.048  Acc: 85.160%\n",
      "Epoch: 448  Loss: 0.048  Acc: 84.994%\n",
      "Epoch: 449  Loss: 0.048  Acc: 85.112%\n",
      "Epoch: 450  Loss: 0.048  Acc: 85.166%\n",
      "Epoch: 451  Loss: 0.048  Acc: 85.242%\n",
      "Epoch: 452  Loss: 0.048  Acc: 85.222%\n",
      "Epoch: 453  Loss: 0.048  Acc: 85.088%\n",
      "Epoch: 454  Loss: 0.048  Acc: 85.276%\n",
      "Epoch: 455  Loss: 0.048  Acc: 85.072%\n",
      "Epoch: 456  Loss: 0.048  Acc: 85.334%\n",
      "Epoch: 457  Loss: 0.048  Acc: 85.226%\n",
      "Epoch: 458  Loss: 0.048  Acc: 85.324%\n",
      "Epoch: 459  Loss: 0.048  Acc: 85.154%\n",
      "Epoch: 460  Loss: 0.048  Acc: 85.202%\n",
      "Epoch: 461  Loss: 0.048  Acc: 85.400%\n",
      "Epoch: 462  Loss: 0.048  Acc: 85.140%\n",
      "Epoch: 463  Loss: 0.048  Acc: 85.334%\n",
      "Epoch: 464  Loss: 0.048  Acc: 85.174%\n",
      "Epoch: 465  Loss: 0.048  Acc: 85.264%\n",
      "Epoch: 466  Loss: 0.048  Acc: 85.276%\n",
      "Epoch: 467  Loss: 0.048  Acc: 85.282%\n",
      "Epoch: 468  Loss: 0.048  Acc: 85.430%\n",
      "Epoch: 469  Loss: 0.048  Acc: 85.150%\n",
      "Epoch: 470  Loss: 0.048  Acc: 85.292%\n",
      "Epoch: 471  Loss: 0.048  Acc: 85.226%\n",
      "Epoch: 472  Loss: 0.048  Acc: 85.384%\n",
      "Epoch: 473  Loss: 0.048  Acc: 85.272%\n",
      "Epoch: 474  Loss: 0.048  Acc: 85.228%\n",
      "Epoch: 475  Loss: 0.047  Acc: 85.412%\n",
      "Epoch: 476  Loss: 0.048  Acc: 85.268%\n",
      "Epoch: 477  Loss: 0.048  Acc: 85.378%\n",
      "Epoch: 478  Loss: 0.047  Acc: 85.240%\n",
      "Epoch: 479  Loss: 0.047  Acc: 85.372%\n",
      "Epoch: 480  Loss: 0.047  Acc: 85.374%\n",
      "Epoch: 481  Loss: 0.047  Acc: 85.462%\n",
      "Epoch: 482  Loss: 0.047  Acc: 85.572%\n",
      "Epoch: 483  Loss: 0.047  Acc: 85.270%\n",
      "Epoch: 484  Loss: 0.047  Acc: 85.566%\n",
      "Epoch: 485  Loss: 0.047  Acc: 85.484%\n",
      "Epoch: 486  Loss: 0.047  Acc: 85.428%\n",
      "Epoch: 487  Loss: 0.047  Acc: 85.508%\n",
      "Epoch: 488  Loss: 0.048  Acc: 85.310%\n",
      "Epoch: 489  Loss: 0.047  Acc: 85.528%\n",
      "Epoch: 490  Loss: 0.048  Acc: 85.238%\n",
      "Epoch: 491  Loss: 0.048  Acc: 85.406%\n",
      "Epoch: 492  Loss: 0.047  Acc: 85.592%\n",
      "Epoch: 493  Loss: 0.048  Acc: 85.450%\n",
      "Epoch: 494  Loss: 0.047  Acc: 85.532%\n",
      "Epoch: 495  Loss: 0.047  Acc: 85.626%\n",
      "Epoch: 496  Loss: 0.047  Acc: 85.658%\n",
      "Epoch: 497  Loss: 0.047  Acc: 85.636%\n",
      "Epoch: 498  Loss: 0.047  Acc: 85.716%\n",
      "Epoch: 499  Loss: 0.047  Acc: 85.506%\n",
      "Epoch: 500  Loss: 0.047  Acc: 85.692%\n",
      "Epoch: 501  Loss: 0.047  Acc: 85.582%\n",
      "Epoch: 502  Loss: 0.047  Acc: 85.830%\n",
      "Epoch: 503  Loss: 0.047  Acc: 85.748%\n",
      "Epoch: 504  Loss: 0.047  Acc: 85.748%\n",
      "Epoch: 505  Loss: 0.047  Acc: 85.774%\n",
      "Epoch: 506  Loss: 0.047  Acc: 85.530%\n",
      "Epoch: 507  Loss: 0.047  Acc: 85.720%\n",
      "Epoch: 508  Loss: 0.047  Acc: 85.546%\n",
      "Epoch: 509  Loss: 0.047  Acc: 85.928%\n",
      "Epoch: 510  Loss: 0.047  Acc: 85.792%\n",
      "Epoch: 511  Loss: 0.047  Acc: 85.842%\n",
      "Epoch: 512  Loss: 0.047  Acc: 85.802%\n",
      "Epoch: 513  Loss: 0.047  Acc: 85.568%\n",
      "Epoch: 514  Loss: 0.047  Acc: 85.570%\n",
      "Epoch: 515  Loss: 0.047  Acc: 85.500%\n",
      "Epoch: 516  Loss: 0.047  Acc: 85.860%\n",
      "Epoch: 517  Loss: 0.047  Acc: 85.916%\n",
      "Epoch: 518  Loss: 0.047  Acc: 85.778%\n",
      "Epoch: 519  Loss: 0.047  Acc: 85.840%\n",
      "Epoch: 520  Loss: 0.047  Acc: 85.592%\n",
      "Epoch: 521  Loss: 0.047  Acc: 85.866%\n",
      "Epoch: 522  Loss: 0.047  Acc: 85.764%\n",
      "Epoch: 523  Loss: 0.047  Acc: 85.856%\n",
      "Epoch: 524  Loss: 0.047  Acc: 85.784%\n",
      "Epoch: 525  Loss: 0.047  Acc: 85.846%\n",
      "Epoch: 526  Loss: 0.047  Acc: 85.840%\n",
      "Epoch: 527  Loss: 0.047  Acc: 85.836%\n",
      "Epoch: 528  Loss: 0.047  Acc: 85.876%\n",
      "Epoch: 529  Loss: 0.047  Acc: 85.938%\n",
      "Epoch: 530  Loss: 0.047  Acc: 85.848%\n",
      "Epoch: 531  Loss: 0.047  Acc: 85.890%\n",
      "Epoch: 532  Loss: 0.047  Acc: 85.782%\n",
      "Epoch: 533  Loss: 0.047  Acc: 85.658%\n",
      "Epoch: 534  Loss: 0.047  Acc: 85.634%\n",
      "Epoch: 535  Loss: 0.047  Acc: 85.612%\n",
      "Epoch: 536  Loss: 0.047  Acc: 85.822%\n",
      "Epoch: 537  Loss: 0.047  Acc: 85.836%\n",
      "Epoch: 538  Loss: 0.047  Acc: 85.938%\n",
      "Epoch: 539  Loss: 0.047  Acc: 85.974%\n",
      "Epoch: 540  Loss: 0.047  Acc: 85.896%\n",
      "Epoch: 541  Loss: 0.047  Acc: 86.000%\n",
      "Epoch: 542  Loss: 0.047  Acc: 85.858%\n",
      "Epoch: 543  Loss: 0.047  Acc: 85.936%\n",
      "Epoch: 544  Loss: 0.047  Acc: 85.792%\n",
      "Epoch: 545  Loss: 0.047  Acc: 85.800%\n",
      "Epoch: 546  Loss: 0.047  Acc: 85.918%\n",
      "Epoch: 547  Loss: 0.047  Acc: 85.844%\n",
      "Epoch: 548  Loss: 0.047  Acc: 85.936%\n",
      "Epoch: 549  Loss: 0.047  Acc: 86.008%\n",
      "Epoch: 550  Loss: 0.047  Acc: 85.984%\n",
      "Epoch: 551  Loss: 0.047  Acc: 85.772%\n",
      "Epoch: 552  Loss: 0.047  Acc: 85.906%\n",
      "Epoch: 553  Loss: 0.047  Acc: 85.566%\n",
      "Epoch: 554  Loss: 0.047  Acc: 85.498%\n",
      "Epoch: 555  Loss: 0.047  Acc: 85.840%\n",
      "Epoch: 556  Loss: 0.047  Acc: 85.620%\n",
      "Epoch: 557  Loss: 0.047  Acc: 85.664%\n",
      "Epoch: 558  Loss: 0.047  Acc: 85.788%\n",
      "Epoch: 559  Loss: 0.047  Acc: 85.654%\n",
      "Epoch: 560  Loss: 0.047  Acc: 85.812%\n",
      "Epoch: 561  Loss: 0.047  Acc: 85.188%\n",
      "Epoch: 562  Loss: 0.047  Acc: 85.216%\n",
      "Epoch: 563  Loss: 0.047  Acc: 85.706%\n",
      "Epoch: 564  Loss: 0.047  Acc: 85.518%\n",
      "Epoch: 565  Loss: 0.047  Acc: 85.776%\n",
      "Epoch: 566  Loss: 0.047  Acc: 85.576%\n",
      "Epoch: 567  Loss: 0.046  Acc: 85.998%\n",
      "Epoch: 568  Loss: 0.047  Acc: 85.774%\n",
      "Epoch: 569  Loss: 0.047  Acc: 85.930%\n",
      "Epoch: 570  Loss: 0.047  Acc: 85.666%\n",
      "Epoch: 571  Loss: 0.047  Acc: 85.754%\n",
      "Epoch: 572  Loss: 0.047  Acc: 85.766%\n",
      "Epoch: 573  Loss: 0.046  Acc: 85.836%\n",
      "Epoch: 574  Loss: 0.047  Acc: 85.884%\n",
      "Epoch: 575  Loss: 0.046  Acc: 86.036%\n",
      "Epoch: 576  Loss: 0.046  Acc: 85.854%\n",
      "Epoch: 577  Loss: 0.046  Acc: 85.836%\n",
      "Epoch: 578  Loss: 0.046  Acc: 85.700%\n",
      "Epoch: 579  Loss: 0.047  Acc: 85.798%\n",
      "Epoch: 580  Loss: 0.046  Acc: 85.948%\n",
      "Epoch: 581  Loss: 0.046  Acc: 85.998%\n",
      "Epoch: 582  Loss: 0.046  Acc: 86.070%\n",
      "Epoch: 583  Loss: 0.046  Acc: 85.978%\n",
      "Epoch: 584  Loss: 0.046  Acc: 85.854%\n",
      "Epoch: 585  Loss: 0.046  Acc: 85.732%\n",
      "Epoch: 586  Loss: 0.046  Acc: 85.808%\n",
      "Epoch: 587  Loss: 0.046  Acc: 85.882%\n",
      "Epoch: 588  Loss: 0.046  Acc: 85.970%\n",
      "Epoch: 589  Loss: 0.046  Acc: 85.966%\n",
      "Epoch: 590  Loss: 0.046  Acc: 85.976%\n",
      "Epoch: 591  Loss: 0.046  Acc: 85.972%\n",
      "Epoch: 592  Loss: 0.046  Acc: 85.932%\n",
      "Epoch: 593  Loss: 0.046  Acc: 85.894%\n",
      "Epoch: 594  Loss: 0.046  Acc: 85.892%\n",
      "Epoch: 595  Loss: 0.046  Acc: 85.896%\n",
      "Epoch: 596  Loss: 0.046  Acc: 85.928%\n",
      "Epoch: 597  Loss: 0.046  Acc: 86.024%\n",
      "Epoch: 598  Loss: 0.046  Acc: 86.118%\n",
      "Epoch: 599  Loss: 0.046  Acc: 86.092%\n",
      "Epoch: 600  Loss: 0.046  Acc: 86.004%\n",
      "Epoch: 601  Loss: 0.046  Acc: 86.042%\n",
      "Epoch: 602  Loss: 0.046  Acc: 85.964%\n",
      "Epoch: 603  Loss: 0.046  Acc: 85.810%\n",
      "Epoch: 604  Loss: 0.046  Acc: 85.960%\n",
      "Epoch: 605  Loss: 0.046  Acc: 85.992%\n",
      "Epoch: 606  Loss: 0.046  Acc: 86.188%\n",
      "Epoch: 607  Loss: 0.046  Acc: 86.058%\n",
      "Epoch: 608  Loss: 0.046  Acc: 86.124%\n",
      "Epoch: 609  Loss: 0.046  Acc: 86.010%\n",
      "Epoch: 610  Loss: 0.046  Acc: 85.972%\n",
      "Epoch: 611  Loss: 0.046  Acc: 85.918%\n",
      "Epoch: 612  Loss: 0.046  Acc: 86.030%\n",
      "Epoch: 613  Loss: 0.046  Acc: 86.018%\n",
      "Epoch: 614  Loss: 0.046  Acc: 86.152%\n",
      "Epoch: 615  Loss: 0.046  Acc: 85.936%\n",
      "Epoch: 616  Loss: 0.046  Acc: 85.942%\n",
      "Epoch: 617  Loss: 0.046  Acc: 85.794%\n",
      "Epoch: 618  Loss: 0.046  Acc: 85.928%\n",
      "Epoch: 619  Loss: 0.046  Acc: 86.060%\n",
      "Epoch: 620  Loss: 0.046  Acc: 86.084%\n",
      "Epoch: 621  Loss: 0.046  Acc: 86.164%\n",
      "Epoch: 622  Loss: 0.046  Acc: 86.054%\n",
      "Epoch: 623  Loss: 0.046  Acc: 86.104%\n",
      "Epoch: 624  Loss: 0.046  Acc: 85.856%\n",
      "Epoch: 625  Loss: 0.046  Acc: 85.834%\n",
      "Epoch: 626  Loss: 0.046  Acc: 85.780%\n",
      "Epoch: 627  Loss: 0.046  Acc: 85.842%\n",
      "Epoch: 628  Loss: 0.046  Acc: 85.940%\n",
      "Epoch: 629  Loss: 0.046  Acc: 85.960%\n",
      "Epoch: 630  Loss: 0.046  Acc: 86.008%\n",
      "Epoch: 631  Loss: 0.046  Acc: 85.842%\n",
      "Epoch: 632  Loss: 0.046  Acc: 86.012%\n",
      "Epoch: 633  Loss: 0.046  Acc: 85.878%\n",
      "Epoch: 634  Loss: 0.046  Acc: 86.146%\n",
      "Epoch: 635  Loss: 0.046  Acc: 86.080%\n",
      "Epoch: 636  Loss: 0.046  Acc: 86.138%\n",
      "Epoch: 637  Loss: 0.046  Acc: 86.086%\n",
      "Epoch: 638  Loss: 0.046  Acc: 85.872%\n",
      "Epoch: 639  Loss: 0.046  Acc: 85.864%\n",
      "Epoch: 640  Loss: 0.046  Acc: 85.888%\n",
      "Epoch: 641  Loss: 0.046  Acc: 86.018%\n",
      "Epoch: 642  Loss: 0.046  Acc: 86.066%\n",
      "Epoch: 643  Loss: 0.046  Acc: 86.082%\n",
      "Epoch: 644  Loss: 0.046  Acc: 86.084%\n",
      "Epoch: 645  Loss: 0.046  Acc: 85.910%\n",
      "Epoch: 646  Loss: 0.046  Acc: 86.056%\n",
      "Epoch: 647  Loss: 0.046  Acc: 85.994%\n",
      "Epoch: 648  Loss: 0.046  Acc: 85.994%\n",
      "Epoch: 649  Loss: 0.046  Acc: 86.056%\n",
      "Epoch: 650  Loss: 0.046  Acc: 85.994%\n",
      "Epoch: 651  Loss: 0.046  Acc: 86.108%\n",
      "Epoch: 652  Loss: 0.046  Acc: 85.922%\n",
      "Epoch: 653  Loss: 0.046  Acc: 85.912%\n",
      "Epoch: 654  Loss: 0.046  Acc: 85.948%\n",
      "Epoch: 655  Loss: 0.046  Acc: 86.032%\n",
      "Epoch: 656  Loss: 0.046  Acc: 86.070%\n",
      "Epoch: 657  Loss: 0.046  Acc: 86.052%\n",
      "Epoch: 658  Loss: 0.046  Acc: 86.042%\n",
      "Epoch: 659  Loss: 0.046  Acc: 85.988%\n",
      "Epoch: 660  Loss: 0.046  Acc: 86.006%\n",
      "Epoch: 661  Loss: 0.046  Acc: 85.960%\n",
      "Epoch: 662  Loss: 0.046  Acc: 86.104%\n",
      "Epoch: 663  Loss: 0.046  Acc: 85.932%\n",
      "Epoch: 664  Loss: 0.046  Acc: 86.012%\n",
      "Epoch: 665  Loss: 0.046  Acc: 85.988%\n",
      "Epoch: 666  Loss: 0.046  Acc: 85.964%\n",
      "Epoch: 667  Loss: 0.046  Acc: 86.056%\n",
      "Epoch: 668  Loss: 0.046  Acc: 86.030%\n",
      "Epoch: 669  Loss: 0.046  Acc: 86.106%\n",
      "Epoch: 670  Loss: 0.046  Acc: 86.186%\n",
      "Epoch: 671  Loss: 0.046  Acc: 86.062%\n",
      "Epoch: 672  Loss: 0.046  Acc: 86.046%\n",
      "Epoch: 673  Loss: 0.046  Acc: 86.000%\n",
      "Epoch: 674  Loss: 0.046  Acc: 85.936%\n",
      "Epoch: 675  Loss: 0.046  Acc: 85.972%\n",
      "Epoch: 676  Loss: 0.046  Acc: 85.898%\n",
      "Epoch: 677  Loss: 0.046  Acc: 86.054%\n",
      "Epoch: 678  Loss: 0.046  Acc: 85.996%\n",
      "Epoch: 679  Loss: 0.046  Acc: 86.054%\n",
      "Epoch: 680  Loss: 0.046  Acc: 86.106%\n",
      "Epoch: 681  Loss: 0.046  Acc: 86.090%\n",
      "Epoch: 682  Loss: 0.046  Acc: 86.078%\n",
      "Epoch: 683  Loss: 0.046  Acc: 86.052%\n",
      "Epoch: 684  Loss: 0.046  Acc: 85.984%\n",
      "Epoch: 685  Loss: 0.046  Acc: 85.932%\n",
      "Epoch: 686  Loss: 0.046  Acc: 85.956%\n",
      "Epoch: 687  Loss: 0.046  Acc: 86.028%\n",
      "Epoch: 688  Loss: 0.046  Acc: 86.132%\n",
      "Epoch: 689  Loss: 0.046  Acc: 86.180%\n",
      "Epoch: 690  Loss: 0.046  Acc: 86.136%\n",
      "Epoch: 691  Loss: 0.046  Acc: 86.054%\n",
      "Epoch: 692  Loss: 0.046  Acc: 85.886%\n",
      "Epoch: 693  Loss: 0.046  Acc: 86.086%\n",
      "Epoch: 694  Loss: 0.046  Acc: 85.960%\n",
      "Epoch: 695  Loss: 0.046  Acc: 86.066%\n",
      "Epoch: 696  Loss: 0.046  Acc: 85.924%\n",
      "Epoch: 697  Loss: 0.046  Acc: 86.198%\n",
      "Epoch: 698  Loss: 0.046  Acc: 86.068%\n",
      "Epoch: 699  Loss: 0.046  Acc: 86.198%\n",
      "Epoch: 700  Loss: 0.046  Acc: 86.102%\n",
      "Epoch: 701  Loss: 0.046  Acc: 86.198%\n",
      "Epoch: 702  Loss: 0.046  Acc: 86.082%\n",
      "Epoch: 703  Loss: 0.046  Acc: 86.058%\n",
      "Epoch: 704  Loss: 0.046  Acc: 86.038%\n",
      "Epoch: 705  Loss: 0.046  Acc: 85.954%\n",
      "Epoch: 706  Loss: 0.046  Acc: 86.058%\n",
      "Epoch: 707  Loss: 0.046  Acc: 86.020%\n",
      "Epoch: 708  Loss: 0.046  Acc: 86.046%\n",
      "Epoch: 709  Loss: 0.046  Acc: 86.174%\n",
      "Epoch: 710  Loss: 0.046  Acc: 86.096%\n",
      "Epoch: 711  Loss: 0.046  Acc: 86.192%\n",
      "Epoch: 712  Loss: 0.046  Acc: 86.092%\n",
      "Epoch: 713  Loss: 0.046  Acc: 86.174%\n",
      "Epoch: 714  Loss: 0.046  Acc: 86.098%\n",
      "Epoch: 715  Loss: 0.046  Acc: 86.162%\n",
      "Epoch: 716  Loss: 0.046  Acc: 86.006%\n",
      "Epoch: 717  Loss: 0.046  Acc: 86.026%\n",
      "Epoch: 718  Loss: 0.046  Acc: 85.878%\n",
      "Epoch: 719  Loss: 0.046  Acc: 86.124%\n",
      "Epoch: 720  Loss: 0.046  Acc: 85.992%\n",
      "Epoch: 721  Loss: 0.046  Acc: 86.178%\n",
      "Epoch: 722  Loss: 0.046  Acc: 86.054%\n",
      "Epoch: 723  Loss: 0.046  Acc: 86.190%\n",
      "Epoch: 724  Loss: 0.046  Acc: 86.228%\n",
      "Epoch: 725  Loss: 0.046  Acc: 86.070%\n",
      "Epoch: 726  Loss: 0.046  Acc: 86.154%\n",
      "Epoch: 727  Loss: 0.046  Acc: 86.006%\n",
      "Epoch: 728  Loss: 0.046  Acc: 86.206%\n",
      "Epoch: 729  Loss: 0.046  Acc: 86.150%\n",
      "Epoch: 730  Loss: 0.046  Acc: 86.098%\n",
      "Epoch: 731  Loss: 0.046  Acc: 86.008%\n",
      "Epoch: 732  Loss: 0.046  Acc: 85.896%\n",
      "Epoch: 733  Loss: 0.046  Acc: 86.028%\n",
      "Epoch: 734  Loss: 0.046  Acc: 86.048%\n",
      "Epoch: 735  Loss: 0.046  Acc: 86.032%\n",
      "Epoch: 736  Loss: 0.046  Acc: 85.904%\n",
      "Epoch: 737  Loss: 0.046  Acc: 85.962%\n",
      "Epoch: 738  Loss: 0.046  Acc: 86.072%\n",
      "Epoch: 739  Loss: 0.046  Acc: 86.184%\n",
      "Epoch: 740  Loss: 0.046  Acc: 86.078%\n",
      "Epoch: 741  Loss: 0.046  Acc: 86.222%\n",
      "Epoch: 742  Loss: 0.046  Acc: 85.904%\n",
      "Epoch: 743  Loss: 0.046  Acc: 86.162%\n",
      "Epoch: 744  Loss: 0.046  Acc: 86.094%\n",
      "Epoch: 745  Loss: 0.046  Acc: 86.066%\n",
      "Epoch: 746  Loss: 0.046  Acc: 86.172%\n",
      "Epoch: 747  Loss: 0.046  Acc: 85.958%\n",
      "Epoch: 748  Loss: 0.046  Acc: 86.152%\n",
      "Epoch: 749  Loss: 0.046  Acc: 86.056%\n",
      "Epoch: 750  Loss: 0.046  Acc: 86.084%\n",
      "Epoch: 751  Loss: 0.046  Acc: 86.154%\n",
      "Epoch: 752  Loss: 0.046  Acc: 86.092%\n",
      "Epoch: 753  Loss: 0.046  Acc: 86.204%\n",
      "Epoch: 754  Loss: 0.046  Acc: 86.030%\n",
      "Epoch: 755  Loss: 0.046  Acc: 86.148%\n",
      "Epoch: 756  Loss: 0.046  Acc: 85.994%\n",
      "Epoch: 757  Loss: 0.046  Acc: 86.172%\n",
      "Epoch: 758  Loss: 0.046  Acc: 86.150%\n",
      "Epoch: 759  Loss: 0.046  Acc: 86.252%\n",
      "Epoch: 760  Loss: 0.046  Acc: 86.088%\n",
      "Epoch: 761  Loss: 0.046  Acc: 86.118%\n",
      "Epoch: 762  Loss: 0.046  Acc: 86.110%\n",
      "Epoch: 763  Loss: 0.046  Acc: 86.004%\n",
      "Epoch: 764  Loss: 0.046  Acc: 86.180%\n",
      "Epoch: 765  Loss: 0.046  Acc: 86.046%\n",
      "Epoch: 766  Loss: 0.046  Acc: 86.142%\n",
      "Epoch: 767  Loss: 0.046  Acc: 86.234%\n",
      "Epoch: 768  Loss: 0.046  Acc: 86.128%\n",
      "Epoch: 769  Loss: 0.046  Acc: 86.106%\n",
      "Epoch: 770  Loss: 0.046  Acc: 85.838%\n",
      "Epoch: 771  Loss: 0.046  Acc: 85.896%\n",
      "Epoch: 772  Loss: 0.046  Acc: 86.044%\n",
      "Epoch: 773  Loss: 0.046  Acc: 86.136%\n",
      "Epoch: 774  Loss: 0.046  Acc: 86.062%\n",
      "Epoch: 775  Loss: 0.046  Acc: 85.940%\n",
      "Epoch: 776  Loss: 0.046  Acc: 86.100%\n",
      "Epoch: 777  Loss: 0.046  Acc: 85.988%\n",
      "Epoch: 778  Loss: 0.046  Acc: 86.026%\n",
      "Epoch: 779  Loss: 0.045  Acc: 86.182%\n",
      "Epoch: 780  Loss: 0.046  Acc: 86.174%\n",
      "Epoch: 781  Loss: 0.046  Acc: 85.846%\n",
      "Epoch: 782  Loss: 0.046  Acc: 85.870%\n",
      "Epoch: 783  Loss: 0.046  Acc: 85.832%\n",
      "Epoch: 784  Loss: 0.046  Acc: 85.940%\n",
      "Epoch: 785  Loss: 0.046  Acc: 85.754%\n",
      "Epoch: 786  Loss: 0.046  Acc: 85.750%\n",
      "Epoch: 787  Loss: 0.046  Acc: 86.124%\n",
      "Epoch: 788  Loss: 0.046  Acc: 85.982%\n",
      "Epoch: 789  Loss: 0.046  Acc: 85.992%\n",
      "Epoch: 790  Loss: 0.046  Acc: 85.872%\n",
      "Epoch: 791  Loss: 0.046  Acc: 86.004%\n",
      "Epoch: 792  Loss: 0.046  Acc: 85.950%\n",
      "Epoch: 793  Loss: 0.046  Acc: 86.118%\n",
      "Epoch: 794  Loss: 0.046  Acc: 86.068%\n",
      "Epoch: 795  Loss: 0.046  Acc: 85.976%\n",
      "Epoch: 796  Loss: 0.046  Acc: 86.110%\n",
      "Epoch: 797  Loss: 0.046  Acc: 85.884%\n",
      "Epoch: 798  Loss: 0.046  Acc: 85.988%\n",
      "Epoch: 799  Loss: 0.046  Acc: 85.946%\n",
      "Epoch: 800  Loss: 0.045  Acc: 86.056%\n",
      "Epoch: 801  Loss: 0.046  Acc: 86.146%\n",
      "Epoch: 802  Loss: 0.045  Acc: 86.188%\n",
      "Epoch: 803  Loss: 0.045  Acc: 86.140%\n",
      "Epoch: 804  Loss: 0.045  Acc: 86.188%\n",
      "Epoch: 805  Loss: 0.046  Acc: 86.082%\n",
      "Epoch: 806  Loss: 0.045  Acc: 86.200%\n",
      "Epoch: 807  Loss: 0.045  Acc: 86.002%\n",
      "Epoch: 808  Loss: 0.045  Acc: 86.194%\n",
      "Epoch: 809  Loss: 0.046  Acc: 86.076%\n",
      "Epoch: 810  Loss: 0.045  Acc: 86.210%\n",
      "Epoch: 811  Loss: 0.045  Acc: 86.076%\n",
      "Epoch: 812  Loss: 0.045  Acc: 86.154%\n",
      "Epoch: 813  Loss: 0.045  Acc: 86.200%\n",
      "Epoch: 814  Loss: 0.045  Acc: 86.050%\n",
      "Epoch: 815  Loss: 0.045  Acc: 86.168%\n",
      "Epoch: 816  Loss: 0.045  Acc: 86.130%\n",
      "Epoch: 817  Loss: 0.046  Acc: 86.026%\n",
      "Epoch: 818  Loss: 0.046  Acc: 86.068%\n",
      "Epoch: 819  Loss: 0.046  Acc: 86.100%\n",
      "Epoch: 820  Loss: 0.046  Acc: 86.032%\n",
      "Epoch: 821  Loss: 0.046  Acc: 86.056%\n",
      "Epoch: 822  Loss: 0.046  Acc: 86.030%\n",
      "Epoch: 823  Loss: 0.046  Acc: 86.078%\n",
      "Epoch: 824  Loss: 0.045  Acc: 86.050%\n",
      "Epoch: 825  Loss: 0.045  Acc: 86.166%\n",
      "Epoch: 826  Loss: 0.045  Acc: 86.186%\n",
      "Epoch: 827  Loss: 0.045  Acc: 86.124%\n",
      "Epoch: 828  Loss: 0.045  Acc: 86.196%\n",
      "Epoch: 829  Loss: 0.045  Acc: 86.172%\n",
      "Epoch: 830  Loss: 0.045  Acc: 86.014%\n",
      "Epoch: 831  Loss: 0.045  Acc: 86.202%\n",
      "Epoch: 832  Loss: 0.045  Acc: 86.108%\n",
      "Epoch: 833  Loss: 0.045  Acc: 86.250%\n",
      "Epoch: 834  Loss: 0.045  Acc: 86.130%\n",
      "Epoch: 835  Loss: 0.045  Acc: 86.240%\n",
      "Epoch: 836  Loss: 0.045  Acc: 86.128%\n",
      "Epoch: 837  Loss: 0.045  Acc: 86.228%\n",
      "Epoch: 838  Loss: 0.045  Acc: 86.132%\n",
      "Epoch: 839  Loss: 0.045  Acc: 86.298%\n",
      "Epoch: 840  Loss: 0.045  Acc: 86.124%\n",
      "Epoch: 841  Loss: 0.045  Acc: 86.224%\n",
      "Epoch: 842  Loss: 0.045  Acc: 86.136%\n",
      "Epoch: 843  Loss: 0.046  Acc: 85.918%\n",
      "Epoch: 844  Loss: 0.045  Acc: 86.132%\n",
      "Epoch: 845  Loss: 0.045  Acc: 86.054%\n",
      "Epoch: 846  Loss: 0.045  Acc: 86.186%\n",
      "Epoch: 847  Loss: 0.045  Acc: 86.176%\n",
      "Epoch: 848  Loss: 0.045  Acc: 86.156%\n",
      "Epoch: 849  Loss: 0.045  Acc: 86.178%\n",
      "Epoch: 850  Loss: 0.045  Acc: 85.922%\n",
      "Epoch: 851  Loss: 0.045  Acc: 86.182%\n",
      "Epoch: 852  Loss: 0.045  Acc: 86.108%\n",
      "Epoch: 853  Loss: 0.045  Acc: 86.252%\n",
      "Epoch: 854  Loss: 0.045  Acc: 86.100%\n",
      "Epoch: 855  Loss: 0.045  Acc: 86.018%\n",
      "Epoch: 856  Loss: 0.045  Acc: 86.092%\n",
      "Epoch: 857  Loss: 0.045  Acc: 86.056%\n",
      "Epoch: 858  Loss: 0.045  Acc: 86.188%\n",
      "Epoch: 859  Loss: 0.045  Acc: 86.180%\n",
      "Epoch: 860  Loss: 0.045  Acc: 86.222%\n",
      "Epoch: 861  Loss: 0.045  Acc: 86.250%\n",
      "Epoch: 862  Loss: 0.045  Acc: 86.112%\n",
      "Epoch: 863  Loss: 0.045  Acc: 86.106%\n",
      "Epoch: 864  Loss: 0.045  Acc: 86.028%\n",
      "Epoch: 865  Loss: 0.046  Acc: 85.888%\n",
      "Epoch: 866  Loss: 0.045  Acc: 86.026%\n",
      "Epoch: 867  Loss: 0.046  Acc: 86.006%\n",
      "Epoch: 868  Loss: 0.045  Acc: 86.036%\n",
      "Epoch: 869  Loss: 0.045  Acc: 86.114%\n",
      "Epoch: 870  Loss: 0.046  Acc: 85.910%\n",
      "Epoch: 871  Loss: 0.045  Acc: 86.070%\n",
      "Epoch: 872  Loss: 0.045  Acc: 86.096%\n",
      "Epoch: 873  Loss: 0.045  Acc: 86.076%\n",
      "Epoch: 874  Loss: 0.045  Acc: 86.212%\n",
      "Epoch: 875  Loss: 0.045  Acc: 86.016%\n",
      "Epoch: 876  Loss: 0.045  Acc: 86.132%\n",
      "Epoch: 877  Loss: 0.046  Acc: 85.944%\n",
      "Epoch: 878  Loss: 0.045  Acc: 86.036%\n",
      "Epoch: 879  Loss: 0.046  Acc: 85.996%\n",
      "Epoch: 880  Loss: 0.046  Acc: 85.924%\n",
      "Epoch: 881  Loss: 0.045  Acc: 86.216%\n",
      "Epoch: 882  Loss: 0.045  Acc: 86.076%\n",
      "Epoch: 883  Loss: 0.045  Acc: 86.184%\n",
      "Epoch: 884  Loss: 0.045  Acc: 86.162%\n",
      "Epoch: 885  Loss: 0.045  Acc: 85.960%\n",
      "Epoch: 886  Loss: 0.046  Acc: 86.100%\n",
      "Epoch: 887  Loss: 0.045  Acc: 86.028%\n",
      "Epoch: 888  Loss: 0.045  Acc: 86.150%\n",
      "Epoch: 889  Loss: 0.045  Acc: 86.174%\n",
      "Epoch: 890  Loss: 0.045  Acc: 86.188%\n",
      "Epoch: 891  Loss: 0.045  Acc: 86.220%\n",
      "Epoch: 892  Loss: 0.045  Acc: 86.102%\n",
      "Epoch: 893  Loss: 0.045  Acc: 86.184%\n",
      "Epoch: 894  Loss: 0.045  Acc: 86.002%\n",
      "Epoch: 895  Loss: 0.045  Acc: 86.054%\n",
      "Epoch: 896  Loss: 0.045  Acc: 86.094%\n",
      "Epoch: 897  Loss: 0.045  Acc: 86.150%\n",
      "Epoch: 898  Loss: 0.045  Acc: 86.220%\n",
      "Epoch: 899  Loss: 0.045  Acc: 86.246%\n",
      "Epoch: 900  Loss: 0.045  Acc: 86.260%\n",
      "Epoch: 901  Loss: 0.045  Acc: 86.098%\n",
      "Epoch: 902  Loss: 0.045  Acc: 86.312%\n",
      "Epoch: 903  Loss: 0.045  Acc: 86.104%\n",
      "Epoch: 904  Loss: 0.045  Acc: 86.272%\n",
      "Epoch: 905  Loss: 0.045  Acc: 86.162%\n",
      "Epoch: 906  Loss: 0.045  Acc: 86.170%\n",
      "Epoch: 907  Loss: 0.045  Acc: 86.250%\n",
      "Epoch: 908  Loss: 0.045  Acc: 86.114%\n",
      "Epoch: 909  Loss: 0.045  Acc: 86.190%\n",
      "Epoch: 910  Loss: 0.045  Acc: 85.938%\n",
      "Epoch: 911  Loss: 0.045  Acc: 86.274%\n",
      "Epoch: 912  Loss: 0.045  Acc: 86.166%\n",
      "Epoch: 913  Loss: 0.045  Acc: 86.278%\n",
      "Epoch: 914  Loss: 0.045  Acc: 86.152%\n",
      "Epoch: 915  Loss: 0.045  Acc: 86.006%\n",
      "Epoch: 916  Loss: 0.045  Acc: 86.168%\n",
      "Epoch: 917  Loss: 0.045  Acc: 86.064%\n",
      "Epoch: 918  Loss: 0.045  Acc: 86.264%\n",
      "Epoch: 919  Loss: 0.045  Acc: 86.190%\n",
      "Epoch: 920  Loss: 0.045  Acc: 86.198%\n",
      "Epoch: 921  Loss: 0.045  Acc: 86.292%\n",
      "Epoch: 922  Loss: 0.045  Acc: 86.098%\n",
      "Epoch: 923  Loss: 0.045  Acc: 86.322%\n",
      "Epoch: 924  Loss: 0.045  Acc: 86.104%\n",
      "Epoch: 925  Loss: 0.045  Acc: 86.330%\n",
      "Epoch: 926  Loss: 0.045  Acc: 86.198%\n",
      "Epoch: 927  Loss: 0.045  Acc: 86.252%\n",
      "Epoch: 928  Loss: 0.045  Acc: 86.266%\n",
      "Epoch: 929  Loss: 0.045  Acc: 86.108%\n",
      "Epoch: 930  Loss: 0.045  Acc: 86.274%\n",
      "Epoch: 931  Loss: 0.045  Acc: 86.142%\n",
      "Epoch: 932  Loss: 0.045  Acc: 86.224%\n",
      "Epoch: 933  Loss: 0.045  Acc: 86.326%\n",
      "Epoch: 934  Loss: 0.045  Acc: 86.136%\n",
      "Epoch: 935  Loss: 0.045  Acc: 86.206%\n",
      "Epoch: 936  Loss: 0.045  Acc: 85.880%\n",
      "Epoch: 937  Loss: 0.045  Acc: 86.350%\n",
      "Epoch: 938  Loss: 0.045  Acc: 86.260%\n",
      "Epoch: 939  Loss: 0.045  Acc: 86.202%\n",
      "Epoch: 940  Loss: 0.045  Acc: 86.218%\n",
      "Epoch: 941  Loss: 0.045  Acc: 86.092%\n",
      "Epoch: 942  Loss: 0.045  Acc: 86.294%\n",
      "Epoch: 943  Loss: 0.045  Acc: 86.196%\n",
      "Epoch: 944  Loss: 0.045  Acc: 86.236%\n",
      "Epoch: 945  Loss: 0.045  Acc: 86.306%\n",
      "Epoch: 946  Loss: 0.045  Acc: 85.936%\n",
      "Epoch: 947  Loss: 0.045  Acc: 86.226%\n",
      "Epoch: 948  Loss: 0.045  Acc: 86.076%\n",
      "Epoch: 949  Loss: 0.045  Acc: 86.028%\n",
      "Epoch: 950  Loss: 0.045  Acc: 86.310%\n",
      "Epoch: 951  Loss: 0.045  Acc: 86.234%\n",
      "Epoch: 952  Loss: 0.045  Acc: 86.132%\n",
      "Epoch: 953  Loss: 0.045  Acc: 86.190%\n",
      "Epoch: 954  Loss: 0.045  Acc: 86.088%\n",
      "Epoch: 955  Loss: 0.045  Acc: 86.128%\n",
      "Epoch: 956  Loss: 0.045  Acc: 85.950%\n",
      "Epoch: 957  Loss: 0.045  Acc: 86.050%\n",
      "Epoch: 958  Loss: 0.045  Acc: 86.200%\n",
      "Epoch: 959  Loss: 0.045  Acc: 86.028%\n",
      "Epoch: 960  Loss: 0.045  Acc: 86.260%\n",
      "Epoch: 961  Loss: 0.045  Acc: 86.214%\n",
      "Epoch: 962  Loss: 0.045  Acc: 86.176%\n",
      "Epoch: 963  Loss: 0.045  Acc: 86.122%\n",
      "Epoch: 964  Loss: 0.045  Acc: 86.016%\n",
      "Epoch: 965  Loss: 0.045  Acc: 86.120%\n",
      "Epoch: 966  Loss: 0.045  Acc: 85.950%\n",
      "Epoch: 967  Loss: 0.045  Acc: 86.282%\n",
      "Epoch: 968  Loss: 0.045  Acc: 86.212%\n",
      "Epoch: 969  Loss: 0.045  Acc: 86.258%\n",
      "Epoch: 970  Loss: 0.045  Acc: 86.286%\n",
      "Epoch: 971  Loss: 0.045  Acc: 86.206%\n",
      "Epoch: 972  Loss: 0.045  Acc: 86.296%\n",
      "Epoch: 973  Loss: 0.045  Acc: 86.098%\n",
      "Epoch: 974  Loss: 0.045  Acc: 86.270%\n",
      "Epoch: 975  Loss: 0.045  Acc: 86.162%\n",
      "Epoch: 976  Loss: 0.045  Acc: 86.142%\n",
      "Epoch: 977  Loss: 0.045  Acc: 86.148%\n",
      "Epoch: 978  Loss: 0.045  Acc: 86.200%\n",
      "Epoch: 979  Loss: 0.045  Acc: 86.204%\n",
      "Epoch: 980  Loss: 0.045  Acc: 86.308%\n",
      "Epoch: 981  Loss: 0.045  Acc: 86.330%\n",
      "Epoch: 982  Loss: 0.045  Acc: 86.286%\n",
      "Epoch: 983  Loss: 0.045  Acc: 86.338%\n",
      "Epoch: 984  Loss: 0.045  Acc: 86.278%\n",
      "Epoch: 985  Loss: 0.045  Acc: 86.122%\n",
      "Epoch: 986  Loss: 0.045  Acc: 86.172%\n",
      "Epoch: 987  Loss: 0.045  Acc: 86.108%\n",
      "Epoch: 988  Loss: 0.045  Acc: 86.062%\n",
      "Epoch: 989  Loss: 0.045  Acc: 86.026%\n",
      "Epoch: 990  Loss: 0.045  Acc: 86.276%\n",
      "Epoch: 991  Loss: 0.045  Acc: 86.014%\n",
      "Epoch: 992  Loss: 0.045  Acc: 86.348%\n",
      "Epoch: 993  Loss: 0.045  Acc: 86.186%\n",
      "Epoch: 994  Loss: 0.045  Acc: 86.120%\n",
      "Epoch: 995  Loss: 0.045  Acc: 86.248%\n",
      "Epoch: 996  Loss: 0.045  Acc: 86.020%\n",
      "Epoch: 997  Loss: 0.045  Acc: 86.130%\n",
      "Epoch: 998  Loss: 0.045  Acc: 85.740%\n",
      "Epoch: 999  Loss: 0.045  Acc: 86.134%\n",
      "Epoch: 1000  Loss: 0.045  Acc: 86.204%\n"
     ]
    }
   ],
   "source": [
    "# train the linear classifier with adam in 1000 examples\n",
    "epoch = 1000\n",
    "best_X = train1(train_img, one_hot_train_lb, epoch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e11efc6",
   "metadata": {},
   "source": [
    "#### 在验证集上测试分类效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3cfb9030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric_Gradient+Adam优化在验证集上的分类精度为: 82.090%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_153644/889779313.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "# test the classification accuracy on validation dataset\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "res=val_img.dot(best_X[0])\n",
    "res=sigmoid(res)\n",
    "res=res.dot(best_X[1])\n",
    "scores = np.argmax(res, axis=1)\n",
    "lb     = np.argmax(one_hot_val_lb,axis=1)\n",
    "accuracy = np.mean(scores == lb) * 100\n",
    "print(\"Numeric_Gradient+Adam优化在验证集上的分类精度为: %.3f%%\" %(accuracy))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8713b35",
   "metadata": {},
   "source": [
    "### 题目2： Train network with bias (Adam) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "165cc0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#在完成上一部分程序的基础上，给两个线性分类器加上bias\n",
    "def train2(x, y, epoch):\n",
    "    \"\"\"\n",
    "    Inputs have dimension D=784, there are C=10 classes, and we operate on N=1000 examples.\n",
    "    \n",
    "    Inputs:\n",
    "    - x: Training images, a numpy array of shape (N, D) containing a minibatch of data.\n",
    "    - y: Training labels, a numpy array of shape (N,C) containing training labels; y[c] = 1 means\n",
    "          that X[i] has label c, where 0 <= c < C.\n",
    "    - epoch: Training iterations, an integer.\n",
    "    \n",
    "    Returns:\n",
    "    - 训练得到的参数\n",
    "    \"\"\"\n",
    "    \n",
    "    # set the hyperparameters\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    # Initialize first and second momentums in Adam\n",
    "    mW1, vW1 = 0, 0\n",
    "    mW2, vW2 = 0, 0\n",
    "    mb1, vb1 = 0, 0\n",
    "    mb2, vb2 = 0, 0\n",
    "\n",
    "    # initialize W1 (D,64) and w2(64,C) \n",
    "    w1 = np.random.randn(x.shape[1], 64) * 0.0001\n",
    "    w2 = np.random.randn(64,y.shape[1]) * 0.0001\n",
    "    b1 = np.random.randn(64 ) * 0.0001\n",
    "    b2 = np.random.randn(10) * 0.0001\n",
    "\n",
    "    for t in range (1,epoch+1):\n",
    "      h = 1 / (1 + np.exp(-((x.dot (w1))) ) )  #(N,64)\n",
    "      h=h+b1\n",
    "      y_pred =h.dot(w2)     #(N,C)\n",
    "      y_pred=y_pred+b2\n",
    "      loss = np.square(y_pred - y).mean( )\n",
    "      scores = np.argmax(y_pred, axis=1)\n",
    "      lb     = np.argmax(y,      axis=1)\n",
    "      accuracy = np.mean(scores == lb) * 100\n",
    "      # compute the numeric gradient\n",
    "      grad_y_pred = (y_pred-y)*2 #Nx10\n",
    "      grad_w2 = h.T.dot(grad_y_pred) #64x10\n",
    "      grad_h = grad_y_pred.dot(w2.T)#50000*64\n",
    "      grad_sig=h*(1-h)*grad_h\n",
    "      grad_w1 = x.T.dot(grad_sig)\n",
    "      grad_b2 =np.sum(grad_y_pred,axis=0)\n",
    "      grad_b1 =np.sum(grad_sig,axis=0)\n",
    "\n",
    "\n",
    "      # update W and b with Adam\n",
    "      grad_w2, mW2, vW2 = adam_opt(grad_w2, mW2, vW2, t)\n",
    "      grad_b2, mb2, vb2 = adam_opt(grad_b2, mb2, vb2, t)\n",
    "      grad_w1, mW1, vW1 = adam_opt(grad_w1, mW1, vW1, t)\n",
    "      grad_b1, mb1, vb1 = adam_opt(grad_b1, mb1, vb1, t)\n",
    "\n",
    "      w2 -= learning_rate*grad_w2\n",
    "      b2 -= learning_rate*grad_b2\n",
    "      w1 -= learning_rate*grad_w1\n",
    "      b1 -= learning_rate*grad_b1\n",
    "      \n",
    "\n",
    "        # print the result\n",
    "      print(\"Epoch: %d  Loss: %.3f  Acc: %.3f%%\" % (t, loss, accuracy))\n",
    "    \n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b2d64e9",
   "metadata": {},
   "source": [
    "#### 在训练集上进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "325a6d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Loss: 0.100  Acc: 9.846%\n",
      "Epoch: 2  Loss: 0.095  Acc: 25.956%\n",
      "Epoch: 3  Loss: 0.091  Acc: 17.508%\n",
      "Epoch: 4  Loss: 0.090  Acc: 12.716%\n",
      "Epoch: 5  Loss: 0.091  Acc: 14.172%\n",
      "Epoch: 6  Loss: 0.092  Acc: 11.230%\n",
      "Epoch: 7  Loss: 0.092  Acc: 11.230%\n",
      "Epoch: 8  Loss: 0.091  Acc: 11.230%\n",
      "Epoch: 9  Loss: 0.090  Acc: 11.230%\n",
      "Epoch: 10  Loss: 0.090  Acc: 11.230%\n",
      "Epoch: 11  Loss: 0.089  Acc: 11.230%\n",
      "Epoch: 12  Loss: 0.089  Acc: 11.230%\n",
      "Epoch: 13  Loss: 0.089  Acc: 11.230%\n",
      "Epoch: 14  Loss: 0.089  Acc: 14.684%\n",
      "Epoch: 15  Loss: 0.088  Acc: 35.000%\n",
      "Epoch: 16  Loss: 0.088  Acc: 49.180%\n",
      "Epoch: 17  Loss: 0.087  Acc: 56.408%\n",
      "Epoch: 18  Loss: 0.086  Acc: 61.904%\n",
      "Epoch: 19  Loss: 0.085  Acc: 64.388%\n",
      "Epoch: 20  Loss: 0.084  Acc: 68.222%\n",
      "Epoch: 21  Loss: 0.083  Acc: 70.786%\n",
      "Epoch: 22  Loss: 0.082  Acc: 71.032%\n",
      "Epoch: 23  Loss: 0.080  Acc: 70.942%\n",
      "Epoch: 24  Loss: 0.079  Acc: 71.520%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_153644/2115636348.py:32: RuntimeWarning: overflow encountered in exp\n",
      "  h = 1 / (1 + np.exp(-((x.dot (w1))) ) )  #(N,64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25  Loss: 0.078  Acc: 72.086%\n",
      "Epoch: 26  Loss: 0.076  Acc: 74.182%\n",
      "Epoch: 27  Loss: 0.075  Acc: 75.454%\n",
      "Epoch: 28  Loss: 0.073  Acc: 77.382%\n",
      "Epoch: 29  Loss: 0.072  Acc: 80.138%\n",
      "Epoch: 30  Loss: 0.070  Acc: 82.184%\n",
      "Epoch: 31  Loss: 0.069  Acc: 83.916%\n",
      "Epoch: 32  Loss: 0.068  Acc: 83.884%\n",
      "Epoch: 33  Loss: 0.066  Acc: 84.492%\n",
      "Epoch: 34  Loss: 0.065  Acc: 85.142%\n",
      "Epoch: 35  Loss: 0.064  Acc: 85.362%\n",
      "Epoch: 36  Loss: 0.063  Acc: 85.718%\n",
      "Epoch: 37  Loss: 0.062  Acc: 86.188%\n",
      "Epoch: 38  Loss: 0.061  Acc: 86.426%\n",
      "Epoch: 39  Loss: 0.060  Acc: 86.402%\n",
      "Epoch: 40  Loss: 0.059  Acc: 86.904%\n",
      "Epoch: 41  Loss: 0.058  Acc: 87.126%\n",
      "Epoch: 42  Loss: 0.057  Acc: 87.250%\n",
      "Epoch: 43  Loss: 0.056  Acc: 87.300%\n",
      "Epoch: 44  Loss: 0.056  Acc: 87.610%\n",
      "Epoch: 45  Loss: 0.055  Acc: 87.610%\n",
      "Epoch: 46  Loss: 0.054  Acc: 87.632%\n",
      "Epoch: 47  Loss: 0.054  Acc: 87.852%\n",
      "Epoch: 48  Loss: 0.053  Acc: 87.922%\n",
      "Epoch: 49  Loss: 0.053  Acc: 87.862%\n",
      "Epoch: 50  Loss: 0.052  Acc: 88.098%\n",
      "Epoch: 51  Loss: 0.052  Acc: 87.950%\n",
      "Epoch: 52  Loss: 0.051  Acc: 88.220%\n",
      "Epoch: 53  Loss: 0.051  Acc: 88.152%\n",
      "Epoch: 54  Loss: 0.050  Acc: 88.236%\n",
      "Epoch: 55  Loss: 0.050  Acc: 88.304%\n",
      "Epoch: 56  Loss: 0.050  Acc: 88.374%\n",
      "Epoch: 57  Loss: 0.049  Acc: 88.266%\n",
      "Epoch: 58  Loss: 0.049  Acc: 88.438%\n",
      "Epoch: 59  Loss: 0.049  Acc: 88.332%\n",
      "Epoch: 60  Loss: 0.049  Acc: 88.552%\n",
      "Epoch: 61  Loss: 0.048  Acc: 88.612%\n",
      "Epoch: 62  Loss: 0.048  Acc: 88.300%\n",
      "Epoch: 63  Loss: 0.048  Acc: 88.708%\n",
      "Epoch: 64  Loss: 0.048  Acc: 88.394%\n",
      "Epoch: 65  Loss: 0.048  Acc: 88.490%\n",
      "Epoch: 66  Loss: 0.048  Acc: 88.558%\n",
      "Epoch: 67  Loss: 0.048  Acc: 88.348%\n",
      "Epoch: 68  Loss: 0.047  Acc: 88.302%\n",
      "Epoch: 69  Loss: 0.047  Acc: 88.266%\n",
      "Epoch: 70  Loss: 0.047  Acc: 88.168%\n",
      "Epoch: 71  Loss: 0.047  Acc: 88.176%\n",
      "Epoch: 72  Loss: 0.047  Acc: 88.080%\n",
      "Epoch: 73  Loss: 0.047  Acc: 88.186%\n",
      "Epoch: 74  Loss: 0.047  Acc: 87.940%\n",
      "Epoch: 75  Loss: 0.047  Acc: 88.134%\n",
      "Epoch: 76  Loss: 0.047  Acc: 87.988%\n",
      "Epoch: 77  Loss: 0.047  Acc: 88.012%\n",
      "Epoch: 78  Loss: 0.047  Acc: 88.130%\n",
      "Epoch: 79  Loss: 0.047  Acc: 88.000%\n",
      "Epoch: 80  Loss: 0.047  Acc: 87.670%\n",
      "Epoch: 81  Loss: 0.047  Acc: 87.986%\n",
      "Epoch: 82  Loss: 0.047  Acc: 87.938%\n",
      "Epoch: 83  Loss: 0.047  Acc: 87.606%\n",
      "Epoch: 84  Loss: 0.047  Acc: 87.936%\n",
      "Epoch: 85  Loss: 0.047  Acc: 87.866%\n",
      "Epoch: 86  Loss: 0.047  Acc: 87.900%\n",
      "Epoch: 87  Loss: 0.047  Acc: 87.628%\n",
      "Epoch: 88  Loss: 0.047  Acc: 87.572%\n",
      "Epoch: 89  Loss: 0.046  Acc: 87.746%\n",
      "Epoch: 90  Loss: 0.046  Acc: 87.772%\n",
      "Epoch: 91  Loss: 0.046  Acc: 87.558%\n",
      "Epoch: 92  Loss: 0.046  Acc: 87.522%\n",
      "Epoch: 93  Loss: 0.046  Acc: 87.142%\n",
      "Epoch: 94  Loss: 0.046  Acc: 87.548%\n",
      "Epoch: 95  Loss: 0.046  Acc: 87.294%\n",
      "Epoch: 96  Loss: 0.046  Acc: 87.326%\n",
      "Epoch: 97  Loss: 0.046  Acc: 87.506%\n",
      "Epoch: 98  Loss: 0.046  Acc: 87.518%\n",
      "Epoch: 99  Loss: 0.045  Acc: 87.632%\n",
      "Epoch: 100  Loss: 0.045  Acc: 87.396%\n"
     ]
    }
   ],
   "source": [
    "# train the linear classifier with adam in 1000 examples\n",
    "epoch = 100\n",
    "best_X = train2(train_img, one_hot_train_lb, epoch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de61ae24",
   "metadata": {},
   "source": [
    "#### 在验证集上测试分类效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a91bf81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric_Gradient+Adam优化在验证集上的分类精度为: 86.930%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_153644/3167757939.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "# test the classification accuracy on validation dataset\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "res=val_img.dot(best_X[0])\n",
    "res+=best_X[1]\n",
    "res=sigmoid(res)\n",
    "res=res.dot(best_X[2])\n",
    "res+=best_X[3]\n",
    "scores = np.argmax(res, axis=1)\n",
    "lb     = np.argmax(one_hot_val_lb,axis=1)\n",
    "accuracy = np.mean(scores == lb) * 100\n",
    "print(\"Numeric_Gradient+Adam优化在验证集上的分类精度为: %.3f%%\" %(accuracy))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0e325ed",
   "metadata": {},
   "source": [
    "### 题目3： 题目2中第二个线性分类器没有激活函数，请为其添加Softmax函数，其他条件不变，完成训练和测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "9cc4d2e5-b3f9-493d-b176-5cb0ff041104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_front(x):\n",
    "    for i in range(x.shape[0]):\n",
    "        x[i]=np.exp(x[i])\n",
    "        x[i]=x[i]/np.sum(x[i])\n",
    "    return x\n",
    "def softmax_back(x,g):\n",
    "    grad=np.zeros(x.shape)\n",
    "    for i in range(x.shape[0]):\n",
    "        grad[i]=x[i]*g[i]-x[i]*(x[i].dot(g[i]))\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "327f201d-6ada-4ddf-9e31-9c332ec74293",
   "metadata": {},
   "outputs": [],
   "source": [
    "#在完成上一部分程序的基础上，给两个线性分类器加上bias\n",
    "def train3(x, y, epoch):\n",
    "    \"\"\"\n",
    "    Inputs have dimension D=784, there are C=10 classes, and we operate on N=1000 examples.\n",
    "    \n",
    "    Inputs:\n",
    "    - x: Training images, a numpy array of shape (N, D) containing a minibatch of data.\n",
    "    - y: Training labels, a numpy array of shape (N,C) containing training labels; y[c] = 1 means\n",
    "          that X[i] has label c, where 0 <= c < C.\n",
    "    - epoch: Training iterations, an integer.\n",
    "    \n",
    "    Returns:\n",
    "    - 训练得到的参数\n",
    "    \"\"\"\n",
    "    \n",
    "    # set the hyperparameters\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    # Initialize first and second momentums in Adam\n",
    "    mW1, vW1 = 0, 0\n",
    "    mW2, vW2 = 0, 0\n",
    "    mb1, vb1 = 0, 0\n",
    "    mb2, vb2 = 0, 0\n",
    "\n",
    "    # initialize W1 (D,64) and w2(64,C) \n",
    "    w1 = np.random.randn(x.shape[1], 64) * 0.0001\n",
    "    w2 = np.random.randn(64,y.shape[1]) * 0.0001\n",
    "    b1 = np.random.randn(64 ) * 0.0001\n",
    "    b2 = np.random.randn(10) * 0.0001\n",
    "\n",
    "    for t in range (1,epoch+1):\n",
    "      h = 1 / (1 + np.exp(-((x.dot (w1))) ) )  #(N,64)\n",
    "      h=h+b1\n",
    "      h2=h.dot(w2)     #(N,C)\n",
    "      h2=h2+b2\n",
    "      y_pred=softmax_front(h2)\n",
    "      loss = np.square(y_pred - y).mean( )\n",
    "      scores = np.argmax(y_pred, axis=1)\n",
    "      lb     = np.argmax(y,      axis=1)\n",
    "      accuracy = np.mean(scores == lb) * 100\n",
    "      # compute the numeric gradient\n",
    "      grad_y_pred = (y_pred-y)*2 #Nx10\n",
    "      grad_h2=softmax_back(h2,grad_y_pred)\n",
    "      grad_w2 = h.T.dot(grad_h2) #64x10\n",
    "      grad_h = grad_h2.dot(w2.T)#50000*64\n",
    "      grad_sig=h*(1-h)*grad_h\n",
    "      grad_w1 = x.T.dot(grad_sig)\n",
    "      grad_b2 =np.sum(grad_y_pred,axis=0)\n",
    "      grad_b1 =np.sum(grad_sig,axis=0)\n",
    "\n",
    "\n",
    "      # update W and b with Adam\n",
    "      grad_w2, mW2, vW2 = adam_opt(grad_w2, mW2, vW2, t)\n",
    "      grad_b2, mb2, vb2 = adam_opt(grad_b2, mb2, vb2, t)\n",
    "      grad_w1, mW1, vW1 = adam_opt(grad_w1, mW1, vW1, t)\n",
    "      grad_b1, mb1, vb1 = adam_opt(grad_b1, mb1, vb1, t)\n",
    "\n",
    "      w2 -= learning_rate*grad_w2\n",
    "      b2 -= learning_rate*grad_b2\n",
    "      w1 -= learning_rate*grad_w1\n",
    "      b1 -= learning_rate*grad_b1\n",
    "      \n",
    "\n",
    "        # print the result\n",
    "      print(\"Epoch: %d  Loss: %.3f  Acc: %.3f%%\" % (t, loss, accuracy))\n",
    "    \n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a750f4d1-a3d6-4c45-8d29-76b0cfdad388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Loss: 0.090  Acc: 10.620%\n",
      "Epoch: 2  Loss: 0.090  Acc: 11.230%\n",
      "Epoch: 3  Loss: 0.090  Acc: 16.860%\n",
      "Epoch: 4  Loss: 0.089  Acc: 38.342%\n",
      "Epoch: 5  Loss: 0.089  Acc: 44.520%\n",
      "Epoch: 6  Loss: 0.089  Acc: 44.372%\n",
      "Epoch: 7  Loss: 0.088  Acc: 47.036%\n",
      "Epoch: 8  Loss: 0.088  Acc: 49.546%\n",
      "Epoch: 9  Loss: 0.087  Acc: 51.024%\n",
      "Epoch: 10  Loss: 0.087  Acc: 52.114%\n",
      "Epoch: 11  Loss: 0.087  Acc: 52.682%\n",
      "Epoch: 12  Loss: 0.086  Acc: 53.866%\n",
      "Epoch: 13  Loss: 0.085  Acc: 55.994%\n",
      "Epoch: 14  Loss: 0.085  Acc: 58.850%\n",
      "Epoch: 15  Loss: 0.084  Acc: 59.860%\n",
      "Epoch: 16  Loss: 0.084  Acc: 60.858%\n",
      "Epoch: 17  Loss: 0.083  Acc: 61.720%\n",
      "Epoch: 18  Loss: 0.083  Acc: 62.200%\n",
      "Epoch: 19  Loss: 0.082  Acc: 62.978%\n",
      "Epoch: 20  Loss: 0.082  Acc: 64.024%\n",
      "Epoch: 21  Loss: 0.081  Acc: 64.742%\n",
      "Epoch: 22  Loss: 0.080  Acc: 65.042%\n",
      "Epoch: 23  Loss: 0.080  Acc: 65.594%\n",
      "Epoch: 24  Loss: 0.079  Acc: 66.120%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_153644/2080053108.py:32: RuntimeWarning: overflow encountered in exp\n",
      "  h = 1 / (1 + np.exp(-((x.dot (w1))) ) )  #(N,64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25  Loss: 0.079  Acc: 66.382%\n",
      "Epoch: 26  Loss: 0.078  Acc: 66.804%\n",
      "Epoch: 27  Loss: 0.077  Acc: 67.288%\n",
      "Epoch: 28  Loss: 0.077  Acc: 67.820%\n",
      "Epoch: 29  Loss: 0.076  Acc: 68.264%\n",
      "Epoch: 30  Loss: 0.076  Acc: 69.752%\n",
      "Epoch: 31  Loss: 0.075  Acc: 71.302%\n",
      "Epoch: 32  Loss: 0.075  Acc: 72.920%\n",
      "Epoch: 33  Loss: 0.074  Acc: 74.648%\n",
      "Epoch: 34  Loss: 0.073  Acc: 74.832%\n",
      "Epoch: 35  Loss: 0.073  Acc: 75.788%\n",
      "Epoch: 36  Loss: 0.072  Acc: 76.654%\n",
      "Epoch: 37  Loss: 0.072  Acc: 76.744%\n",
      "Epoch: 38  Loss: 0.071  Acc: 77.532%\n",
      "Epoch: 39  Loss: 0.071  Acc: 77.880%\n",
      "Epoch: 40  Loss: 0.070  Acc: 78.056%\n",
      "Epoch: 41  Loss: 0.070  Acc: 78.744%\n",
      "Epoch: 42  Loss: 0.070  Acc: 78.996%\n",
      "Epoch: 43  Loss: 0.069  Acc: 79.418%\n",
      "Epoch: 44  Loss: 0.069  Acc: 79.814%\n",
      "Epoch: 45  Loss: 0.068  Acc: 79.572%\n",
      "Epoch: 46  Loss: 0.068  Acc: 79.794%\n",
      "Epoch: 47  Loss: 0.067  Acc: 79.964%\n",
      "Epoch: 48  Loss: 0.067  Acc: 79.796%\n",
      "Epoch: 49  Loss: 0.067  Acc: 79.950%\n",
      "Epoch: 50  Loss: 0.066  Acc: 80.270%\n",
      "Epoch: 51  Loss: 0.066  Acc: 80.364%\n",
      "Epoch: 52  Loss: 0.066  Acc: 80.420%\n",
      "Epoch: 53  Loss: 0.065  Acc: 80.230%\n",
      "Epoch: 54  Loss: 0.065  Acc: 80.286%\n",
      "Epoch: 55  Loss: 0.065  Acc: 80.336%\n",
      "Epoch: 56  Loss: 0.064  Acc: 80.280%\n",
      "Epoch: 57  Loss: 0.064  Acc: 80.152%\n",
      "Epoch: 58  Loss: 0.064  Acc: 80.338%\n",
      "Epoch: 59  Loss: 0.064  Acc: 80.368%\n",
      "Epoch: 60  Loss: 0.063  Acc: 80.346%\n",
      "Epoch: 61  Loss: 0.063  Acc: 80.490%\n",
      "Epoch: 62  Loss: 0.063  Acc: 80.442%\n",
      "Epoch: 63  Loss: 0.063  Acc: 80.530%\n",
      "Epoch: 64  Loss: 0.062  Acc: 80.380%\n",
      "Epoch: 65  Loss: 0.062  Acc: 80.212%\n",
      "Epoch: 66  Loss: 0.062  Acc: 80.244%\n",
      "Epoch: 67  Loss: 0.061  Acc: 80.224%\n",
      "Epoch: 68  Loss: 0.061  Acc: 80.124%\n",
      "Epoch: 69  Loss: 0.061  Acc: 80.436%\n",
      "Epoch: 70  Loss: 0.060  Acc: 80.130%\n",
      "Epoch: 71  Loss: 0.060  Acc: 80.096%\n",
      "Epoch: 72  Loss: 0.060  Acc: 80.204%\n",
      "Epoch: 73  Loss: 0.059  Acc: 80.078%\n",
      "Epoch: 74  Loss: 0.059  Acc: 80.264%\n",
      "Epoch: 75  Loss: 0.059  Acc: 80.468%\n",
      "Epoch: 76  Loss: 0.059  Acc: 80.510%\n",
      "Epoch: 77  Loss: 0.058  Acc: 80.616%\n",
      "Epoch: 78  Loss: 0.058  Acc: 80.632%\n",
      "Epoch: 79  Loss: 0.058  Acc: 80.796%\n",
      "Epoch: 80  Loss: 0.057  Acc: 81.012%\n",
      "Epoch: 81  Loss: 0.057  Acc: 80.714%\n",
      "Epoch: 82  Loss: 0.057  Acc: 80.668%\n",
      "Epoch: 83  Loss: 0.057  Acc: 80.816%\n",
      "Epoch: 84  Loss: 0.056  Acc: 81.044%\n",
      "Epoch: 85  Loss: 0.056  Acc: 81.164%\n",
      "Epoch: 86  Loss: 0.056  Acc: 80.886%\n",
      "Epoch: 87  Loss: 0.056  Acc: 80.542%\n",
      "Epoch: 88  Loss: 0.055  Acc: 80.594%\n",
      "Epoch: 89  Loss: 0.055  Acc: 80.814%\n",
      "Epoch: 90  Loss: 0.055  Acc: 80.670%\n",
      "Epoch: 91  Loss: 0.055  Acc: 80.278%\n",
      "Epoch: 92  Loss: 0.055  Acc: 80.496%\n",
      "Epoch: 93  Loss: 0.054  Acc: 80.784%\n",
      "Epoch: 94  Loss: 0.054  Acc: 80.700%\n",
      "Epoch: 95  Loss: 0.054  Acc: 80.688%\n",
      "Epoch: 96  Loss: 0.054  Acc: 80.820%\n",
      "Epoch: 97  Loss: 0.053  Acc: 81.032%\n",
      "Epoch: 98  Loss: 0.053  Acc: 81.050%\n",
      "Epoch: 99  Loss: 0.053  Acc: 80.988%\n",
      "Epoch: 100  Loss: 0.053  Acc: 80.974%\n"
     ]
    }
   ],
   "source": [
    "epoch = 100\n",
    "best_X = train3(train_img, one_hot_train_lb, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0e3df79a-c356-4e83-b67d-1b719bf78322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric_Gradient+Adam优化在验证集上的分类精度为: 80.320%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_153644/1531931008.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "res=val_img.dot(best_X[0])\n",
    "res+=best_X[1]\n",
    "res=sigmoid(res)\n",
    "res=res.dot(best_X[2])\n",
    "res+=best_X[3]\n",
    "res=softmax_front(res)\n",
    "scores = np.argmax(res, axis=1)\n",
    "lb     = np.argmax(one_hot_val_lb,axis=1)\n",
    "accuracy = np.mean(scores == lb) * 100\n",
    "print(\"Numeric_Gradient+Adam优化在验证集上的分类精度为: %.3f%%\" %(accuracy))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59159f16-8a93-4ab1-8e18-e46dd0a20d31",
   "metadata": {},
   "source": [
    "第一个:\n",
    "Epoch: 1000  Loss: 0.045  Acc: 86.204%\n",
    "Numeric_Gradient+Adam优化在验证集上的分类精度为: 82.090%\n",
    "加了偏置第二个：\n",
    "Epoch: 100  Loss: 0.045  Acc: 87.396%\n",
    "Numeric_Gradient+Adam优化在验证集上的分类精度为: 86.930%\n",
    "加softmax第三个:\n",
    "Epoch: 100  Loss: 0.053  Acc: 80.974%\n",
    "Numeric_Gradient+Adam优化在验证集上的分类精度为: 80.320%\n",
    "\n",
    "3个的准确度差不多都在80%左右\n",
    "2和3在训练次数过多时loss降低但是准确度也降低\n",
    "需要调整loss的计算方法 加入正则项等方法"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
